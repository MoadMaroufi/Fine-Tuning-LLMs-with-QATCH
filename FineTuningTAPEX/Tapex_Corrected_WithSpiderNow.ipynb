{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btkx8OspZyTF",
        "outputId": "ed22c0d4-3445-45c5-9e6d-b33db9809507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 216124, done.\u001b[K\n",
            "remote: Counting objects: 100% (1005/1005), done.\u001b[K\n",
            "remote: Compressing objects: 100% (635/635), done.\u001b[K\n",
            "remote: Total 216124 (delta 516), reused 655 (delta 298), pack-reused 215119\u001b[K\n",
            "Receiving objects: 100% (216124/216124), 227.40 MiB | 12.10 MiB/s, done.\n",
            "Resolving deltas: 100% (156178/156178), done.\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m339.9/339.9 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 15.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/huggingface/transformers.git\n",
        "!pip install --quiet datasets\n",
        "!pip install --quiet qatch\n",
        "!pip install --quiet transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet transformers[torch]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWOTSneVttXL",
        "outputId": "d3aa67b9-514c-4232-c150-e966b360ff92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoDhuGOPaUPZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import logging\n",
        "import warnings\n",
        "from qatch import MetricEvaluator\n",
        "from qatch.database_reader import MultipleDatabases\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HDPN35TOwE9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d69c5ae-feb1-4211-efff-c37d288a75dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "os.chdir(\"/content/transformers/examples/research_projects/tapex\")##only for training\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title get_sql_tag\n",
        "def get_sql_query_level(sql_query):\n",
        "    # Function to classify SQL query and return just the level tag\n",
        "    def classify_sql_query_level(sql_query):\n",
        "        sql_query_lower = sql_query.lower()\n",
        "        level = 6  # Default to OTHER\n",
        "\n",
        "        if \"select\" in sql_query_lower and \"from\" in sql_query_lower:\n",
        "            level = 0  # SELECT\n",
        "            if \"max(\" in sql_query_lower or \"min(\" in sql_query_lower or \\\n",
        "               \"avg(\" in sql_query_lower or \"count(\" in sql_query_lower or \\\n",
        "               \"sum(\" in sql_query_lower:\n",
        "                level = 1  # SIMPLE_AGGR\n",
        "\n",
        "        if \"where\" in sql_query_lower:\n",
        "            level = 2  # WHERE\n",
        "\n",
        "        if \"group by\" in sql_query_lower:\n",
        "            level = 3  # GROUPBY\n",
        "\n",
        "        if \"having\" in sql_query_lower:\n",
        "            level = 4  # HAVING\n",
        "\n",
        "        if \"order by\" in sql_query_lower:\n",
        "            level = 5  # ORDERBY\n",
        "\n",
        "        return level\n",
        "\n",
        "    # Classification tags\n",
        "    classification_tags = {\n",
        "        0: \"SELECT\",\n",
        "        1: \"SIMPLE_AGGR\",\n",
        "        2: \"WHERE\",\n",
        "        3: \"GROUPBY\",\n",
        "        4: \"HAVING\",\n",
        "        5: \"ORDERBY\",\n",
        "        6: \"OTHER\"\n",
        "    }\n",
        "\n",
        "    # Classify the SQL query\n",
        "    level = classify_sql_query_level(sql_query)\n",
        "\n",
        "    # Return the classification tag\n",
        "    return classification_tags[level]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8H3O5yx8iKzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/spider/db_table_to_df_valid.pkl', 'rb') as file:\n",
        "    db_to_df_spiderDev = pickle.load(file)"
      ],
      "metadata": {
        "id": "yMNWjXzVjNvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spider_test_pd = pd.read_pickle('/content/drive/MyDrive/spider/SPIDER_Simple_cleaned_final_valid.pkl')\n",
        "spider_test_pd['sql_tags'] = spider_test_pd['query'].apply(get_sql_query_level)"
      ],
      "metadata": {
        "id": "WKmtg9dciS27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wuI7ZGUR3CW"
      },
      "outputs": [],
      "source": [
        "db_save_path = '/content/drive/MyDrive/ProprietaryDatasets/database4'\n",
        "databases = MultipleDatabases(db_save_path)\n",
        "evaluator = MetricEvaluator(databases=databases)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For spider Dev\n",
        "db_save_path_spiderDev = '/content/drive/MyDrive/spider/test_database'\n",
        "databasesSpider_Dev = MultipleDatabases(db_save_path_spiderDev)\n",
        "evaluator = MetricEvaluator(databases=databasesSpider_Dev)"
      ],
      "metadata": {
        "id": "Q8nsCG4jppeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14RU_c2TOVfb"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/ProprietaryDatasetsDict.pkl', 'rb') as f:\n",
        "      db_to_df = pickle.load(f)\n",
        "qatch_pickle = pd.read_pickle('/content/drive/MyDrive/spider/QATCH_proprietary_v2.pkl')\n",
        "ProprietaryDomains=['medicine','miscellaneous','ecommerce','finance']\n",
        "qatch_pickle=qatch_pickle[qatch_pickle['domain']==ProprietaryDomains[0]]\n",
        "qatch_pickle['sql_tags'] = qatch_pickle['sql_tags'].apply(lambda x: x.split('-')[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_IyiB7-OPkm",
        "outputId": "031173c7-c7c5-4151-9190-dd40e8e4c888"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total dataset size: 429\n",
            "Training set size: 344\n",
            "Tempset size: 85\n"
          ]
        }
      ],
      "source": [
        "percentage_valid = 0.2\n",
        "temp_indices = qatch_pickle.groupby('sql_tags', group_keys=False).apply(lambda x: x.sample(frac=percentage_valid, random_state=42)).index\n",
        "temp_df = qatch_pickle.loc[temp_indices]\n",
        "train_df_full = qatch_pickle.drop(temp_indices)\n",
        "\n",
        "# Check the size of each set to confirm the split\n",
        "print(f\"Total dataset size: {qatch_pickle.shape[0]}\")\n",
        "print(f\"Training set size: {train_df_full.shape[0]}\")\n",
        "print(f\"Tempset size: {temp_df.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBVpx-dSIRHv",
        "outputId": "b61c8825-72f0-41cc-e522-4a1d8bf162ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total dataset size: 85\n",
            "Valid set size: 42\n",
            "Test size: 43\n"
          ]
        }
      ],
      "source": [
        "percentage_test = 0.5\n",
        "test_indices = temp_df.groupby('sql_tags', group_keys=False).apply(lambda x: x.sample(frac=percentage_test, random_state=42)).index\n",
        "test_df = qatch_pickle.loc[test_indices]\n",
        "valid_df = temp_df.drop(test_indices)\n",
        "\n",
        "# Check the size of each set to confirm the split\n",
        "print(f\"Total dataset size: {temp_df.shape[0]}\")\n",
        "print(f\"Valid set size: {valid_df.shape[0]}\")\n",
        "print(f\"Test size: {test_df.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now lets sample from teh trainset as we are not going to use all of it\n",
        "percentage_train = 0.1\n",
        "train_indices = train_df_full.groupby('sql_tags', group_keys=False).apply(lambda x: x.sample(frac=percentage_train, random_state=42)).index\n",
        "train_df = train_df_full.loc[train_indices]\n",
        "\n",
        "# Check the size of each set to confirm the split\n",
        "print(f\"Total dataset size: {train_df_full.shape[0]}\")\n",
        "print(f\"Training set size: {train_df.shape[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kshSFHT1i_CA",
        "outputId": "91b6214d-210e-4a76-fdb5-e5a6c5df6883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total dataset size: 344\n",
            "Training set size: 34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Hkfd5enEW9nx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d996554f-e8d2-41cb-d09d-27567cc65c2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing my_script.py\n"
          ]
        }
      ],
      "source": [
        "#@title run_model_wikisql\n",
        "%%writefile my_script.py\n",
        "#!/usr/bin/env python\n",
        "# coding=utf-8\n",
        "# Copyright 2022 The Microsoft and The HuggingFace Inc. team. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"\n",
        "Fine-tuning the library models for tapex on table-based question answering tasks.\n",
        "Adapted from script: https://github.com/huggingface/transformers/blob/master/examples/pytorch/summarization/run_summarization.py\n",
        "\"\"\"\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass, field\n",
        "from functools import partial\n",
        "from typing import List, Optional\n",
        "\n",
        "import nltk  # Here to have a nice missing dependency error message early on\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from filelock import FileLock\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    BartForConditionalGeneration,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    HfArgumentParser,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    TapexTokenizer,\n",
        "    set_seed,\n",
        ")\n",
        "from transformers.file_utils import is_offline_mode\n",
        "from transformers.trainer_utils import get_last_checkpoint, is_main_process\n",
        "from transformers.utils import check_min_version\n",
        "\n",
        "\n",
        "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
        "check_min_version(\"4.17.0.dev0\")\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "try:\n",
        "    nltk.data.find(\"tokenizers/punkt\")\n",
        "except (LookupError, OSError):\n",
        "    if is_offline_mode():\n",
        "        raise LookupError(\n",
        "            \"Offline mode: run this script without TRANSFORMERS_OFFLINE first to download nltk data files\"\n",
        "        )\n",
        "    with FileLock(\".lock\") as lock:\n",
        "        nltk.download(\"punkt\", quiet=True)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
        "    \"\"\"\n",
        "\n",
        "    model_name_or_path: str = field(\n",
        "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"},\n",
        "    )\n",
        "    config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
        "    )\n",
        "    tokenizer_name: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Pretrained tokenizer name or path if not the same as model_name. \"\n",
        "                \"By default we use BART-large tokenizer for TAPEX-large.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    cache_dir: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"Where to store the pretrained models downloaded from huggingface.co\"},\n",
        "    )\n",
        "    use_fast_tokenizer: bool = field(\n",
        "        default=True,\n",
        "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
        "    )\n",
        "    model_revision: str = field(\n",
        "        default=\"main\",\n",
        "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
        "    )\n",
        "    use_auth_token: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n",
        "                \"with private models).\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataTrainingArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
        "    \"\"\"\n",
        "\n",
        "    dataset_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
        "    )\n",
        "    dataset_config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
        "    )\n",
        "    train_file: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The input training data file (a jsonlines or csv file).\"}\n",
        "    )\n",
        "    validation_file: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"An optional input evaluation data file to evaluate the metrics (rouge) on (a jsonlines or csv file).\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    test_file: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"An optional input test data file to evaluate the metrics (rouge) on (a jsonlines or csv file).\"\n",
        "        },\n",
        "    )\n",
        "    overwrite_cache: bool = field(\n",
        "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
        "    )\n",
        "    preprocessing_num_workers: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
        "    )\n",
        "    max_source_length: Optional[int] = field(\n",
        "        default=1024,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "                \"than this will be truncated, sequences shorter will be padded.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_target_length: Optional[int] = field(\n",
        "        default=128,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The maximum total sequence length for target text after tokenization. Sequences longer \"\n",
        "                \"than this will be truncated, sequences shorter will be padded.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    val_max_target_length: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The maximum total sequence length for validation target text after tokenization. Sequences longer \"\n",
        "                \"than this will be truncated, sequences shorter will be padded. Will default to `max_target_length`. \"\n",
        "                \"This argument is also used to override the ``max_length`` param of ``model.generate``, which is used \"\n",
        "                \"during ``evaluate`` and ``predict``.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    pad_to_max_length: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Whether to pad all samples to model maximum sentence length. \"\n",
        "                \"If False, will pad the samples dynamically when batching to the maximum length in the batch. More \"\n",
        "                \"efficient on GPU but very bad for TPU.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_train_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_eval_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_predict_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    num_beams: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Number of beams to use for evaluation. This argument will be passed to ``model.generate``, \"\n",
        "                \"which is used during ``evaluate`` and ``predict``.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    ignore_pad_token_for_loss: bool = field(\n",
        "        default=True,\n",
        "        metadata={\n",
        "            \"help\": \"Whether to ignore the tokens corresponding to padded labels in the loss computation or not.\"\n",
        "        },\n",
        "    )\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n",
        "            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n",
        "        else:\n",
        "            if self.train_file is not None:\n",
        "                extension = self.train_file.split(\".\")[-1]\n",
        "                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
        "            if self.validation_file is not None:\n",
        "                extension = self.validation_file.split(\".\")[-1]\n",
        "                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n",
        "        if self.val_max_target_length is None:\n",
        "            self.val_max_target_length = self.max_target_length\n",
        "\n",
        "\n",
        "def main():\n",
        "    # See all possible arguments in src/transformers/training_args.py\n",
        "    # or by passing the --help flag to this script.\n",
        "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
        "\n",
        "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, Seq2SeqTrainingArguments))\n",
        "    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
        "        # If we pass only one argument to the script and it's the path to a json file,\n",
        "        # let's parse it to get our arguments.\n",
        "        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
        "    else:\n",
        "        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
        "\n",
        "    # Detecting last checkpoint.\n",
        "    last_checkpoint = None\n",
        "    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
        "        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
        "        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
        "            raise ValueError(\n",
        "                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
        "                \"Use --overwrite_output_dir to overcome.\"\n",
        "            )\n",
        "        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
        "            logger.info(\n",
        "                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
        "                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
        "            )\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        handlers=[logging.StreamHandler(sys.stdout)],\n",
        "    )\n",
        "    logger.setLevel(logging.INFO if is_main_process(training_args.local_rank) else logging.WARN)\n",
        "\n",
        "    # Log on each process the small summary:\n",
        "    logger.warning(\n",
        "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
        "        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
        "    )\n",
        "    # Set the verbosity to info of the Transformers logger (on main process only):\n",
        "    if is_main_process(training_args.local_rank):\n",
        "        transformers.utils.logging.set_verbosity_info()\n",
        "    logger.info(f\"Training/evaluation parameters {training_args}\")\n",
        "\n",
        "    # Set seed before initializing model.\n",
        "    set_seed(training_args.seed)\n",
        "\n",
        "    # Get the datasets: you can either provide your own CSV/JSON training and evaluation files (see below)\n",
        "    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n",
        "    # (the dataset will be downloaded automatically from the datasets Hub).\n",
        "    #\n",
        "    # For JSON files, this script will use the `question` column for the input question and `table` column for the corresponding table.\n",
        "    #\n",
        "    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n",
        "    # download the dataset.\n",
        "    if data_args.dataset_name is not None:\n",
        "        # Downloading and loading a dataset from the hub.\n",
        "        datasets = load_dataset(data_args.dataset_name, data_args.dataset_config_name, cache_dir=model_args.cache_dir)\n",
        "    else:\n",
        "        data_files = {}\n",
        "        if data_args.train_file is not None:\n",
        "            data_files[\"train\"] = data_args.train_file\n",
        "            extension = data_args.train_file.split(\".\")[-1]\n",
        "        if data_args.validation_file is not None:\n",
        "            data_files[\"validation\"] = data_args.validation_file\n",
        "            extension = data_args.validation_file.split(\".\")[-1]\n",
        "        if data_args.test_file is not None:\n",
        "            data_files[\"test\"] = data_args.test_file\n",
        "            extension = data_args.test_file.split(\".\")[-1]\n",
        "        datasets = load_dataset(extension, data_files=data_files, cache_dir=model_args.cache_dir)\n",
        "\n",
        "    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n",
        "    # https://huggingface.co/docs/datasets/loading_datasets.\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    #\n",
        "    # Distributed training:\n",
        "    # The .from_pretrained methods guarantee that only one local process can concurrently\n",
        "    # download model & vocab.\n",
        "\n",
        "    config = AutoConfig.from_pretrained(\n",
        "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
        "        cache_dir=model_args.cache_dir,\n",
        "        revision=model_args.model_revision,\n",
        "        token=True if model_args.use_auth_token else None,\n",
        "    )\n",
        "\n",
        "    # IMPORTANT: the initial BART model's decoding is penalized by no_repeat_ngram_size, and thus\n",
        "    # we should disable it here to avoid problematic generation\n",
        "    config.no_repeat_ngram_size = 0\n",
        "    config.max_length = 1024\n",
        "    config.early_stopping = False\n",
        "\n",
        "    # load tapex tokenizer\n",
        "    tokenizer = TapexTokenizer.from_pretrained(\n",
        "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
        "        cache_dir=model_args.cache_dir,\n",
        "        use_fast=model_args.use_fast_tokenizer,\n",
        "        revision=model_args.model_revision,\n",
        "        token=True if model_args.use_auth_token else None,\n",
        "        add_prefix_space=True,\n",
        "    )\n",
        "\n",
        "    # load Bart based Tapex model (default tapex-large)\n",
        "    model = BartForConditionalGeneration.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
        "        config=config,\n",
        "        cache_dir=model_args.cache_dir,\n",
        "        revision=model_args.model_revision,\n",
        "        token=True if model_args.use_auth_token else None,\n",
        "    )\n",
        "\n",
        "    if model.config.decoder_start_token_id is None:\n",
        "        raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n",
        "\n",
        "    # Preprocessing the datasets.\n",
        "    # We need to tokenize inputs and targets.\n",
        "    if training_args.do_train:\n",
        "        column_names = datasets[\"train\"].column_names\n",
        "    elif training_args.do_eval:\n",
        "        column_names = datasets[\"validation\"].column_names\n",
        "    elif training_args.do_predict:\n",
        "        column_names = datasets[\"test\"].column_names\n",
        "    else:\n",
        "        logger.info(\"There is nothing to do. Please pass `do_train`, `do_eval` and/or `do_predict`.\")\n",
        "        return\n",
        "\n",
        "    # Temporarily set max_target_length for training.\n",
        "    max_target_length = data_args.max_target_length\n",
        "    padding = \"max_length\" if data_args.pad_to_max_length else False\n",
        "\n",
        "    if training_args.label_smoothing_factor > 0 and not hasattr(model, \"prepare_decoder_input_ids_from_labels\"):\n",
        "        logger.warning(\n",
        "            \"label_smoothing is enabled but the `prepare_decoder_input_ids_from_labels` method is not defined for \"\n",
        "            f\"`{model.__class__.__name__}`. This will lead to loss being calculated twice and will take up more memory\"\n",
        "        )\n",
        "\n",
        "    def preprocess_tableqa_function(examples, is_training=False):\n",
        "        \"\"\"\n",
        "        The is_training FLAG is used to identify if we could use the supervision\n",
        "        to truncate the table content if it is required.\n",
        "        \"\"\"\n",
        "\n",
        "        questions = [question.lower() for question in examples[\"question\"]]\n",
        "        example_tables = examples[\"table\"]\n",
        "        tables = [\n",
        "            pd.DataFrame.from_records(example_table[\"rows\"], columns=example_table[\"header\"])\n",
        "            for example_table in example_tables\n",
        "        ]\n",
        "\n",
        "        # using wikitablequestion's answer set\n",
        "        answers = examples[\"answers\"]\n",
        "\n",
        "        # IMPORTANT: we cannot pass by answers during evaluation, answers passed during training are used to\n",
        "        # truncate large tables in the train set!\n",
        "        if is_training:\n",
        "            model_inputs = tokenizer(\n",
        "                table=tables,\n",
        "                query=questions,\n",
        "                answer=answers,\n",
        "                max_length=data_args.max_source_length,\n",
        "                padding=padding,\n",
        "                truncation=True,\n",
        "            )\n",
        "        else:\n",
        "            model_inputs = tokenizer(\n",
        "                table=tables, query=questions, max_length=data_args.max_source_length, padding=padding, truncation=True\n",
        "            )\n",
        "\n",
        "        labels = tokenizer(\n",
        "            answer=[\", \".join(answer) for answer in answers],\n",
        "            max_length=max_target_length,\n",
        "            padding=padding,\n",
        "            truncation=True,\n",
        "        )\n",
        "\n",
        "        # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
        "        # padding in the loss.\n",
        "        if padding == \"max_length\" and data_args.ignore_pad_token_for_loss:\n",
        "            labels[\"input_ids\"] = [\n",
        "                [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
        "            ]\n",
        "\n",
        "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "    # in training, we can use the answer as extra information to truncate large tables\n",
        "    preprocess_tableqa_function_training = partial(preprocess_tableqa_function, is_training=True)\n",
        "\n",
        "    if training_args.do_train:\n",
        "        if \"train\" not in datasets:\n",
        "            raise ValueError(\"--do_train requires a train dataset\")\n",
        "        train_dataset = datasets[\"train\"]\n",
        "        if data_args.max_train_samples is not None:\n",
        "            train_dataset = train_dataset.select(range(data_args.max_train_samples))\n",
        "        train_dataset = train_dataset.map(\n",
        "            preprocess_tableqa_function_training,\n",
        "            batched=True,\n",
        "            num_proc=data_args.preprocessing_num_workers,\n",
        "            remove_columns=column_names,\n",
        "            load_from_cache_file=not data_args.overwrite_cache,\n",
        "        )\n",
        "\n",
        "    if training_args.do_eval:\n",
        "        max_target_length = data_args.val_max_target_length\n",
        "        if \"validation\" not in datasets:\n",
        "            raise ValueError(\"--do_eval requires a validation dataset\")\n",
        "        eval_dataset = datasets[\"validation\"]\n",
        "        if data_args.max_eval_samples is not None:\n",
        "            eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))\n",
        "        eval_dataset = eval_dataset.map(\n",
        "            preprocess_tableqa_function,\n",
        "            batched=True,\n",
        "            num_proc=data_args.preprocessing_num_workers,\n",
        "            remove_columns=column_names,\n",
        "            load_from_cache_file=not data_args.overwrite_cache,\n",
        "        )\n",
        "\n",
        "    if training_args.do_predict:\n",
        "        max_target_length = data_args.val_max_target_length\n",
        "        if \"test\" not in datasets:\n",
        "            raise ValueError(\"--do_predict requires a test dataset\")\n",
        "        predict_dataset = datasets[\"test\"]\n",
        "        if data_args.max_predict_samples is not None:\n",
        "            predict_dataset = predict_dataset.select(range(data_args.max_predict_samples))\n",
        "        predict_dataset = predict_dataset.map(\n",
        "            preprocess_tableqa_function,\n",
        "            batched=True,\n",
        "            num_proc=data_args.preprocessing_num_workers,\n",
        "            remove_columns=column_names,\n",
        "            load_from_cache_file=not data_args.overwrite_cache,\n",
        "        )\n",
        "\n",
        "    # Data collator\n",
        "    label_pad_token_id = -100 if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n",
        "    data_collator = DataCollatorForSeq2Seq(\n",
        "        tokenizer,\n",
        "        model=model,\n",
        "        label_pad_token_id=label_pad_token_id,\n",
        "        pad_to_multiple_of=8 if training_args.fp16 else None,\n",
        "    )\n",
        "\n",
        "    def postprocess_text(preds, labels):\n",
        "        preds = [pred.strip() for pred in preds]\n",
        "        labels = [label.strip() for label in labels]\n",
        "\n",
        "        return preds, labels\n",
        "\n",
        "    def compute_metrics(eval_preds):\n",
        "        preds, labels = eval_preds\n",
        "        if isinstance(preds, tuple):\n",
        "            preds = preds[0]\n",
        "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "        if data_args.ignore_pad_token_for_loss:\n",
        "            # Replace -100 in the labels as we can't decode them.\n",
        "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "        # Some simple post-processing\n",
        "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "        delimiter = \", \"\n",
        "\n",
        "        # define example evaluation\n",
        "        def evaluate_example(predict_str: str, ground_str: str):\n",
        "            predict_spans = predict_str.split(delimiter)\n",
        "            ground_spans = ground_str.split(delimiter)\n",
        "            predict_values = defaultdict(lambda: 0)\n",
        "            ground_values = defaultdict(lambda: 0)\n",
        "            for span in predict_spans:\n",
        "                try:\n",
        "                    predict_values[float(span)] += 1\n",
        "                except ValueError:\n",
        "                    predict_values[span.strip()] += 1\n",
        "            for span in ground_spans:\n",
        "                try:\n",
        "                    ground_values[float(span)] += 1\n",
        "                except ValueError:\n",
        "                    ground_values[span.strip()] += 1\n",
        "            _is_correct = predict_values == ground_values\n",
        "            return _is_correct\n",
        "\n",
        "        def get_denotation_accuracy(predictions: List[str], references: List[str]):\n",
        "            assert len(predictions) == len(references)\n",
        "            correct_num = 0\n",
        "            for predict_str, ground_str in zip(predictions, references):\n",
        "                is_correct = evaluate_example(predict_str.lower(), ground_str.lower())\n",
        "                if is_correct:\n",
        "                    correct_num += 1\n",
        "            return correct_num / len(predictions)\n",
        "\n",
        "        accuracy = get_denotation_accuracy(decoded_preds, decoded_labels)\n",
        "        result = {\"denotation_accuracy\": accuracy}\n",
        "\n",
        "        return result\n",
        "\n",
        "    # Initialize our Trainer\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset if training_args.do_train else None,\n",
        "        eval_dataset=eval_dataset if training_args.do_eval else None,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics if training_args.predict_with_generate else None,\n",
        "    )\n",
        "\n",
        "    if training_args.do_train:\n",
        "        checkpoint = None\n",
        "        if training_args.resume_from_checkpoint is not None:\n",
        "            checkpoint = training_args.resume_from_checkpoint\n",
        "        elif last_checkpoint is not None:\n",
        "            checkpoint = last_checkpoint\n",
        "        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
        "        trainer.save_model()  # Saves the tokenizer too for easy upload\n",
        "\n",
        "        metrics = train_result.metrics\n",
        "        max_train_samples = (\n",
        "            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
        "        )\n",
        "        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
        "\n",
        "        trainer.log_metrics(\"train\", metrics)\n",
        "        trainer.save_metrics(\"train\", metrics)\n",
        "        trainer.save_state()\n",
        "\n",
        "    # Evaluation\n",
        "    results = {}\n",
        "    if training_args.do_eval:\n",
        "        logger.info(\"*** Evaluate ***\")\n",
        "\n",
        "        metrics = trainer.evaluate(\n",
        "            max_length=data_args.val_max_target_length, num_beams=data_args.num_beams, metric_key_prefix=\"eval\"\n",
        "        )\n",
        "        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n",
        "        metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
        "\n",
        "        trainer.log_metrics(\"eval\", metrics)\n",
        "        trainer.save_metrics(\"eval\", metrics)\n",
        "\n",
        "    if training_args.do_predict:\n",
        "        logger.info(\"*** Predict ***\")\n",
        "\n",
        "        predict_results = trainer.predict(\n",
        "            predict_dataset,\n",
        "            metric_key_prefix=\"predict\",\n",
        "            max_length=data_args.val_max_target_length,\n",
        "            num_beams=data_args.num_beams,\n",
        "        )\n",
        "        metrics = predict_results.metrics\n",
        "        max_predict_samples = (\n",
        "            data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)\n",
        "        )\n",
        "        metrics[\"predict_samples\"] = min(max_predict_samples, len(predict_dataset))\n",
        "\n",
        "        trainer.log_metrics(\"predict\", metrics)\n",
        "        trainer.save_metrics(\"predict\", metrics)\n",
        "\n",
        "        if trainer.is_world_process_zero():\n",
        "            if training_args.predict_with_generate:\n",
        "                predictions = tokenizer.batch_decode(\n",
        "                    predict_results.predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "                )\n",
        "                predictions = [pred.strip() for pred in predictions]\n",
        "                output_prediction_file = os.path.join(training_args.output_dir, \"tapex_predictions.txt\")\n",
        "                with open(output_prediction_file, \"w\") as writer:\n",
        "                    writer.write(\"\\n\".join(predictions))\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def _mp_fn(index):\n",
        "    # For xla_spawn (TPUs)\n",
        "    main()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # --max_steps 600 \\Remove this and put 5 epochs"
      ],
      "metadata": {
        "id": "DYGg2f6J01WO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5tRynG_aDl4",
        "outputId": "683c5121-96dc-4463-ba26-24abb34f8e98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-06-19 11:42:10.437602: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-06-19 11:42:10.437656: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-06-19 11:42:10.445272: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-06-19 11:42:10.466747: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-06-19 11:42:12.752399: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "06/19/2024 11:42:15 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
            "06/19/2024 11:42:15 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_steps=200,\n",
            "eval_strategy=steps,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.1,\n",
            "learning_rate=3e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/runs/Jun19_11-42-15_50a2f422cf33,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=10,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=/content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=200,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=200,\n",
            "weight_decay=0.01,\n",
            ")\n",
            "Generating train split: 2306 examples [00:00, 9795.91 examples/s]\n",
            "Generating validation split: 287 examples [00:00, 31881.06 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--tapex-base/snapshots/3e7cafc9cd54d57f7b2209478ec84fdae8c5f73b/config.json\n",
            "Model config BartConfig {\n",
            "  \"_name_or_path\": \"microsoft/tapex-base\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"BartForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--microsoft--tapex-base/snapshots/3e7cafc9cd54d57f7b2209478ec84fdae8c5f73b/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--microsoft--tapex-base/snapshots/3e7cafc9cd54d57f7b2209478ec84fdae8c5f73b/merges.txt\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--tapex-base/snapshots/3e7cafc9cd54d57f7b2209478ec84fdae8c5f73b/tokenizer_config.json\n",
            "loading file tokenizer.json from cache at None\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--tapex-base/snapshots/3e7cafc9cd54d57f7b2209478ec84fdae8c5f73b/model.safetensors\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"max_length\": 1024,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
            "\n",
            "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at microsoft/tapex-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--tapex-base/snapshots/3e7cafc9cd54d57f7b2209478ec84fdae8c5f73b/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "Map: 100% 2306/2306 [00:17<00:00, 131.09 examples/s]\n",
            "Map: 100% 287/287 [00:02<00:00, 143.14 examples/s]\n",
            "***** Running training *****\n",
            "  Num examples = 2,306\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1,445\n",
            "  Number of trainable parameters = 139,420,416\n",
            "{'loss': 4.8928, 'grad_norm': 42.20134353637695, 'learning_rate': 1.5e-06, 'epoch': 0.03}\n",
            "{'loss': 4.1958, 'grad_norm': 10.018514633178711, 'learning_rate': 3e-06, 'epoch': 0.07}\n",
            "{'loss': 3.4035, 'grad_norm': 8.983917236328125, 'learning_rate': 4.5e-06, 'epoch': 0.1}\n",
            "{'loss': 3.0583, 'grad_norm': 9.86195182800293, 'learning_rate': 6e-06, 'epoch': 0.14}\n",
            "{'loss': 2.6772, 'grad_norm': 7.433486461639404, 'learning_rate': 7.5e-06, 'epoch': 0.17}\n",
            "{'loss': 2.5202, 'grad_norm': 13.853589057922363, 'learning_rate': 9e-06, 'epoch': 0.21}\n",
            "{'loss': 2.5519, 'grad_norm': 15.958466529846191, 'learning_rate': 1.05e-05, 'epoch': 0.24}\n",
            "{'loss': 2.4089, 'grad_norm': 10.109740257263184, 'learning_rate': 1.2e-05, 'epoch': 0.28}\n",
            "{'loss': 2.3071, 'grad_norm': 10.128434181213379, 'learning_rate': 1.3500000000000001e-05, 'epoch': 0.31}\n",
            "{'loss': 2.3361, 'grad_norm': 8.78346061706543, 'learning_rate': 1.5e-05, 'epoch': 0.35}\n",
            "{'loss': 2.1911, 'grad_norm': 7.154712677001953, 'learning_rate': 1.65e-05, 'epoch': 0.38}\n",
            "{'loss': 2.1693, 'grad_norm': 4.596778392791748, 'learning_rate': 1.8e-05, 'epoch': 0.42}\n",
            "{'loss': 2.1376, 'grad_norm': 3.9909214973449707, 'learning_rate': 1.95e-05, 'epoch': 0.45}\n",
            "{'loss': 2.0925, 'grad_norm': 4.911707878112793, 'learning_rate': 2.1e-05, 'epoch': 0.48}\n",
            "{'loss': 2.1457, 'grad_norm': 9.43020248413086, 'learning_rate': 2.25e-05, 'epoch': 0.52}\n",
            "{'loss': 2.0217, 'grad_norm': 15.613028526306152, 'learning_rate': 2.4e-05, 'epoch': 0.55}\n",
            "{'loss': 2.0948, 'grad_norm': 14.160400390625, 'learning_rate': 2.55e-05, 'epoch': 0.59}\n",
            "{'loss': 2.0024, 'grad_norm': 7.676178932189941, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.62}\n",
            "{'loss': 2.0449, 'grad_norm': 10.401864051818848, 'learning_rate': 2.8499999999999998e-05, 'epoch': 0.66}\n",
            "{'loss': 1.9457, 'grad_norm': 10.659796714782715, 'learning_rate': 3e-05, 'epoch': 0.69}\n",
            " 14% 200/1445 [02:31<15:59,  1.30it/s]***** Running Evaluation *****\n",
            "  Num examples = 287\n",
            "  Batch size = 8\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"max_length\": 1024,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1283: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "\n",
            "  0% 0/36 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/36 [00:02<00:43,  1.27s/it]\u001b[A\n",
            "  8% 3/36 [00:10<02:20,  4.25s/it]\u001b[A\n",
            " 11% 4/36 [00:16<02:29,  4.66s/it]\u001b[A\n",
            " 14% 5/36 [00:23<02:48,  5.44s/it]\u001b[A\n",
            " 17% 6/36 [00:25<02:11,  4.37s/it]\u001b[A\n",
            " 19% 7/36 [00:29<02:08,  4.42s/it]\u001b[A\n",
            " 22% 8/36 [00:33<01:52,  4.02s/it]\u001b[A\n",
            " 25% 9/36 [00:36<01:39,  3.70s/it]\u001b[A\n",
            " 28% 10/36 [00:38<01:29,  3.43s/it]\u001b[A\n",
            " 31% 11/36 [00:46<01:54,  4.60s/it]\u001b[A\n",
            " 33% 12/36 [00:50<01:47,  4.46s/it]\u001b[A\n",
            " 36% 13/36 [00:59<02:15,  5.88s/it]\u001b[A\n",
            " 39% 14/36 [01:06<02:17,  6.25s/it]\u001b[A\n",
            " 42% 15/36 [01:10<01:56,  5.57s/it]\u001b[A\n",
            " 44% 16/36 [01:20<02:19,  6.96s/it]\u001b[A\n",
            " 47% 17/36 [01:31<02:31,  7.96s/it]\u001b[A\n",
            " 50% 18/36 [01:32<01:50,  6.14s/it]\u001b[A\n",
            " 53% 19/36 [01:35<01:25,  5.01s/it]\u001b[A\n",
            " 56% 20/36 [01:36<01:00,  3.81s/it]\u001b[A\n",
            " 58% 21/36 [01:41<01:03,  4.26s/it]\u001b[A\n",
            " 61% 22/36 [01:46<01:00,  4.29s/it]\u001b[A\n",
            " 64% 23/36 [01:53<01:09,  5.35s/it]\u001b[A\n",
            " 67% 24/36 [02:03<01:21,  6.77s/it]\u001b[A\n",
            " 69% 25/36 [02:09<01:11,  6.50s/it]\u001b[A\n",
            " 72% 26/36 [02:39<02:14, 13.43s/it]\u001b[A\n",
            " 75% 27/36 [02:43<01:35, 10.63s/it]\u001b[A\n",
            " 78% 28/36 [02:53<01:23, 10.46s/it]\u001b[A\n",
            " 81% 29/36 [03:31<02:10, 18.66s/it]\u001b[A\n",
            " 83% 30/36 [03:37<01:29, 14.95s/it]\u001b[A\n",
            " 86% 31/36 [03:45<01:03, 12.73s/it]\u001b[A\n",
            " 89% 32/36 [03:48<00:39,  9.84s/it]\u001b[A\n",
            " 92% 33/36 [03:54<00:26,  8.82s/it]\u001b[A\n",
            " 94% 34/36 [04:01<00:16,  8.18s/it]\u001b[A\n",
            " 97% 35/36 [04:10<00:08,  8.58s/it]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.9962983131408691, 'eval_denotation_accuracy': 0.1916376306620209, 'eval_runtime': 288.494, 'eval_samples_per_second': 0.995, 'eval_steps_per_second': 0.125, 'epoch': 0.69}\n",
            " 14% 200/1445 [07:20<15:59,  1.30it/s]\n",
            "100% 36/36 [04:26<00:00,  8.35s/it]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-200\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 1024, 'num_beams': 4, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Configuration saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-200/config.json\n",
            "Configuration saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-200/generation_config.json\n",
            "Model weights saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-200/model.safetensors\n",
            "tokenizer config file saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-200/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-200/special_tokens_map.json\n",
            "{'loss': 2.1552, 'grad_norm': 10.95508098602295, 'learning_rate': 2.9759036144578315e-05, 'epoch': 0.73}\n",
            "{'loss': 1.9413, 'grad_norm': 7.641047477722168, 'learning_rate': 2.951807228915663e-05, 'epoch': 0.76}\n",
            "{'loss': 2.0038, 'grad_norm': 7.20764684677124, 'learning_rate': 2.927710843373494e-05, 'epoch': 0.8}\n",
            "{'loss': 1.9603, 'grad_norm': 6.4984893798828125, 'learning_rate': 2.9036144578313254e-05, 'epoch': 0.83}\n",
            "{'loss': 1.9819, 'grad_norm': 6.22639274597168, 'learning_rate': 2.8795180722891565e-05, 'epoch': 0.87}\n",
            "{'loss': 1.8872, 'grad_norm': 7.717649936676025, 'learning_rate': 2.855421686746988e-05, 'epoch': 0.9}\n",
            "{'loss': 1.8519, 'grad_norm': 2.6738927364349365, 'learning_rate': 2.8313253012048193e-05, 'epoch': 0.93}\n",
            "{'loss': 1.9303, 'grad_norm': 7.676196575164795, 'learning_rate': 2.8072289156626508e-05, 'epoch': 0.97}\n",
            "{'loss': 1.9594, 'grad_norm': 9.133225440979004, 'learning_rate': 2.783132530120482e-05, 'epoch': 1.0}\n",
            "{'loss': 1.807, 'grad_norm': 3.760131359100342, 'learning_rate': 2.7590361445783133e-05, 'epoch': 1.04}\n",
            "{'loss': 1.834, 'grad_norm': 4.750338554382324, 'learning_rate': 2.7349397590361447e-05, 'epoch': 1.07}\n",
            "{'loss': 1.8163, 'grad_norm': 3.960374116897583, 'learning_rate': 2.710843373493976e-05, 'epoch': 1.11}\n",
            "{'loss': 1.8919, 'grad_norm': 6.670115947723389, 'learning_rate': 2.6867469879518075e-05, 'epoch': 1.14}\n",
            "{'loss': 1.7304, 'grad_norm': 10.505537986755371, 'learning_rate': 2.6626506024096386e-05, 'epoch': 1.18}\n",
            "{'loss': 1.8179, 'grad_norm': 5.05607795715332, 'learning_rate': 2.6385542168674697e-05, 'epoch': 1.21}\n",
            "{'loss': 1.8415, 'grad_norm': 2.6998586654663086, 'learning_rate': 2.614457831325301e-05, 'epoch': 1.25}\n",
            "{'loss': 1.8164, 'grad_norm': 4.250572681427002, 'learning_rate': 2.5903614457831325e-05, 'epoch': 1.28}\n",
            "{'loss': 1.773, 'grad_norm': 4.637941360473633, 'learning_rate': 2.566265060240964e-05, 'epoch': 1.31}\n",
            "{'loss': 1.85, 'grad_norm': 7.159910678863525, 'learning_rate': 2.542168674698795e-05, 'epoch': 1.35}\n",
            "{'loss': 1.8448, 'grad_norm': 7.1653923988342285, 'learning_rate': 2.5180722891566265e-05, 'epoch': 1.38}\n",
            " 28% 400/1445 [09:59<12:54,  1.35it/s]***** Running Evaluation *****\n",
            "  Num examples = 287\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/36 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/36 [00:02<00:35,  1.06s/it]\u001b[A\n",
            "  8% 3/36 [00:04<00:47,  1.43s/it]\u001b[A\n",
            " 11% 4/36 [00:06<01:01,  1.92s/it]\u001b[A\n",
            " 14% 5/36 [00:09<01:03,  2.04s/it]\u001b[A\n",
            " 17% 6/36 [00:11<01:03,  2.13s/it]\u001b[A\n",
            " 19% 7/36 [00:14<01:08,  2.38s/it]\u001b[A\n",
            " 22% 8/36 [00:16<01:07,  2.42s/it]\u001b[A\n",
            " 25% 9/36 [00:18<00:59,  2.21s/it]\u001b[A\n",
            " 28% 10/36 [00:20<00:55,  2.14s/it]\u001b[A\n",
            " 31% 11/36 [00:22<00:54,  2.20s/it]\u001b[A\n",
            " 33% 12/36 [00:25<00:55,  2.32s/it]\u001b[A\n",
            " 36% 13/36 [00:31<01:21,  3.54s/it]\u001b[A\n",
            " 39% 14/36 [00:35<01:17,  3.54s/it]\u001b[A\n",
            " 42% 15/36 [00:39<01:16,  3.66s/it]\u001b[A\n",
            " 44% 16/36 [00:45<01:30,  4.54s/it]\u001b[A\n",
            " 47% 17/36 [00:52<01:35,  5.02s/it]\u001b[A\n",
            " 50% 18/36 [00:52<01:05,  3.66s/it]\u001b[A\n",
            " 53% 19/36 [00:53<00:46,  2.74s/it]\u001b[A\n",
            " 56% 20/36 [00:53<00:34,  2.17s/it]\u001b[A\n",
            " 58% 21/36 [00:54<00:25,  1.71s/it]\u001b[A\n",
            " 61% 22/36 [00:55<00:20,  1.44s/it]\u001b[A\n",
            " 64% 23/36 [01:01<00:36,  2.79s/it]\u001b[A\n",
            " 67% 24/36 [01:03<00:31,  2.65s/it]\u001b[A\n",
            " 69% 25/36 [01:10<00:42,  3.90s/it]\u001b[A\n",
            " 72% 26/36 [01:13<00:37,  3.76s/it]\u001b[A\n",
            " 75% 27/36 [01:16<00:29,  3.31s/it]\u001b[A\n",
            " 78% 28/36 [01:18<00:23,  2.94s/it]\u001b[A\n",
            " 81% 29/36 [01:21<00:20,  2.96s/it]\u001b[A\n",
            " 83% 30/36 [01:23<00:16,  2.73s/it]\u001b[A\n",
            " 86% 31/36 [01:30<00:20,  4.13s/it]\u001b[A\n",
            " 89% 32/36 [01:32<00:13,  3.42s/it]\u001b[A\n",
            " 92% 33/36 [01:34<00:09,  3.06s/it]\u001b[A\n",
            " 94% 34/36 [01:41<00:08,  4.17s/it]\u001b[A\n",
            " 97% 35/36 [01:43<00:03,  3.54s/it]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.7915380001068115, 'eval_denotation_accuracy': 0.3344947735191638, 'eval_runtime': 114.4357, 'eval_samples_per_second': 2.508, 'eval_steps_per_second': 0.315, 'epoch': 1.38}\n",
            " 28% 400/1445 [11:53<12:54,  1.35it/s]\n",
            "100% 36/36 [01:53<00:00,  2.79s/it]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-400\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 1024, 'num_beams': 4, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Configuration saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-400/config.json\n",
            "Configuration saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-400/generation_config.json\n",
            "Model weights saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-400/model.safetensors\n",
            "tokenizer config file saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-400/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-400/special_tokens_map.json\n",
            "{'loss': 1.8539, 'grad_norm': 5.107031345367432, 'learning_rate': 2.493975903614458e-05, 'epoch': 1.42}\n",
            "{'loss': 1.6921, 'grad_norm': 3.421748399734497, 'learning_rate': 2.4698795180722893e-05, 'epoch': 1.45}\n",
            "{'loss': 1.721, 'grad_norm': 4.989448547363281, 'learning_rate': 2.4457831325301207e-05, 'epoch': 1.49}\n",
            "{'loss': 1.7107, 'grad_norm': 5.108428955078125, 'learning_rate': 2.421686746987952e-05, 'epoch': 1.52}\n",
            "{'loss': 1.8196, 'grad_norm': 2.4541876316070557, 'learning_rate': 2.397590361445783e-05, 'epoch': 1.56}\n",
            "{'loss': 1.8548, 'grad_norm': 9.529118537902832, 'learning_rate': 2.3734939759036143e-05, 'epoch': 1.59}\n",
            "{'loss': 1.7903, 'grad_norm': 7.6274847984313965, 'learning_rate': 2.3493975903614457e-05, 'epoch': 1.63}\n",
            "{'loss': 1.7369, 'grad_norm': 9.231077194213867, 'learning_rate': 2.325301204819277e-05, 'epoch': 1.66}\n",
            "{'loss': 1.7524, 'grad_norm': 3.925565242767334, 'learning_rate': 2.3012048192771086e-05, 'epoch': 1.7}\n",
            "{'loss': 1.7289, 'grad_norm': 3.864335298538208, 'learning_rate': 2.2771084337349397e-05, 'epoch': 1.73}\n",
            "{'loss': 1.7302, 'grad_norm': 4.641659259796143, 'learning_rate': 2.253012048192771e-05, 'epoch': 1.76}\n",
            "{'loss': 1.8249, 'grad_norm': 5.2522759437561035, 'learning_rate': 2.2289156626506025e-05, 'epoch': 1.8}\n",
            "{'loss': 1.7344, 'grad_norm': 5.267579078674316, 'learning_rate': 2.204819277108434e-05, 'epoch': 1.83}\n",
            "{'loss': 1.7067, 'grad_norm': 5.474658966064453, 'learning_rate': 2.1807228915662654e-05, 'epoch': 1.87}\n",
            "{'loss': 1.779, 'grad_norm': 6.170783042907715, 'learning_rate': 2.1566265060240964e-05, 'epoch': 1.9}\n",
            "{'loss': 1.7488, 'grad_norm': 12.055485725402832, 'learning_rate': 2.1325301204819275e-05, 'epoch': 1.94}\n",
            "{'loss': 1.7497, 'grad_norm': 5.503271579742432, 'learning_rate': 2.108433734939759e-05, 'epoch': 1.97}\n",
            "{'loss': 1.7254, 'grad_norm': 3.007779836654663, 'learning_rate': 2.0843373493975904e-05, 'epoch': 2.01}\n",
            "{'loss': 1.6686, 'grad_norm': 6.792949676513672, 'learning_rate': 2.0602409638554218e-05, 'epoch': 2.04}\n",
            "{'loss': 1.6635, 'grad_norm': 7.281795024871826, 'learning_rate': 2.0361445783132532e-05, 'epoch': 2.08}\n",
            " 42% 600/1445 [14:41<11:10,  1.26it/s]***** Running Evaluation *****\n",
            "  Num examples = 287\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/36 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/36 [00:02<00:36,  1.09s/it]\u001b[A\n",
            "  8% 3/36 [00:03<00:34,  1.05s/it]\u001b[A\n",
            " 11% 4/36 [00:05<00:53,  1.68s/it]\u001b[A\n",
            " 14% 5/36 [00:08<00:58,  1.87s/it]\u001b[A\n",
            " 17% 6/36 [00:09<00:55,  1.86s/it]\u001b[A\n",
            " 19% 7/36 [00:13<01:06,  2.31s/it]\u001b[A\n",
            " 22% 8/36 [00:15<01:04,  2.30s/it]\u001b[A\n",
            " 25% 9/36 [00:17<00:57,  2.13s/it]\u001b[A\n",
            " 28% 10/36 [00:19<00:56,  2.16s/it]\u001b[A\n",
            " 31% 11/36 [00:22<01:01,  2.45s/it]\u001b[A\n",
            " 33% 12/36 [00:25<00:58,  2.45s/it]\u001b[A\n",
            " 36% 13/36 [00:31<01:24,  3.68s/it]\u001b[A\n",
            " 39% 14/36 [00:35<01:21,  3.71s/it]\u001b[A\n",
            " 42% 15/36 [00:39<01:19,  3.81s/it]\u001b[A\n",
            " 44% 16/36 [00:43<01:17,  3.87s/it]\u001b[A\n",
            " 47% 17/36 [00:47<01:12,  3.80s/it]\u001b[A\n",
            " 50% 18/36 [00:47<00:50,  2.81s/it]\u001b[A\n",
            " 53% 19/36 [00:48<00:37,  2.20s/it]\u001b[A\n",
            " 56% 20/36 [00:49<00:28,  1.79s/it]\u001b[A\n",
            " 58% 21/36 [00:49<00:21,  1.46s/it]\u001b[A\n",
            " 61% 22/36 [00:50<00:18,  1.33s/it]\u001b[A\n",
            " 64% 23/36 [00:54<00:26,  2.06s/it]\u001b[A\n",
            " 67% 24/36 [00:56<00:24,  2.08s/it]\u001b[A\n",
            " 69% 25/36 [01:03<00:36,  3.32s/it]\u001b[A\n",
            " 72% 26/36 [01:05<00:31,  3.19s/it]\u001b[A\n",
            " 75% 27/36 [01:08<00:26,  2.92s/it]\u001b[A\n",
            " 78% 28/36 [01:10<00:21,  2.66s/it]\u001b[A\n",
            " 81% 29/36 [01:13<00:19,  2.80s/it]\u001b[A\n",
            " 83% 30/36 [01:15<00:15,  2.52s/it]\u001b[A\n",
            " 86% 31/36 [01:18<00:13,  2.68s/it]\u001b[A\n",
            " 89% 32/36 [01:20<00:10,  2.60s/it]\u001b[A\n",
            " 92% 33/36 [01:23<00:07,  2.59s/it]\u001b[A\n",
            " 94% 34/36 [01:29<00:07,  3.65s/it]\u001b[A\n",
            " 97% 35/36 [01:30<00:02,  2.97s/it]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.7647260427474976, 'eval_denotation_accuracy': 0.43205574912891986, 'eval_runtime': 100.9462, 'eval_samples_per_second': 2.843, 'eval_steps_per_second': 0.357, 'epoch': 2.08}\n",
            " 42% 600/1445 [16:22<11:10,  1.26it/s]\n",
            "100% 36/36 [01:40<00:00,  2.42s/it]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-600\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 1024, 'num_beams': 4, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Configuration saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-600/config.json\n",
            "Configuration saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-600/generation_config.json\n",
            "Model weights saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-600/model.safetensors\n",
            "tokenizer config file saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-600/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-600/special_tokens_map.json\n",
            "{'loss': 1.6562, 'grad_norm': 5.265618801116943, 'learning_rate': 2.0120481927710843e-05, 'epoch': 2.11}\n",
            "{'loss': 1.6495, 'grad_norm': 6.2099385261535645, 'learning_rate': 1.9879518072289157e-05, 'epoch': 2.15}\n",
            "{'loss': 1.6643, 'grad_norm': 7.51587438583374, 'learning_rate': 1.963855421686747e-05, 'epoch': 2.18}\n",
            "{'loss': 1.6712, 'grad_norm': 5.888816833496094, 'learning_rate': 1.9397590361445785e-05, 'epoch': 2.21}\n",
            "{'loss': 1.7205, 'grad_norm': 3.9558427333831787, 'learning_rate': 1.9156626506024096e-05, 'epoch': 2.25}\n",
            "{'loss': 1.7075, 'grad_norm': 5.823276042938232, 'learning_rate': 1.891566265060241e-05, 'epoch': 2.28}\n",
            "{'loss': 1.6496, 'grad_norm': 5.254728317260742, 'learning_rate': 1.867469879518072e-05, 'epoch': 2.32}\n",
            "{'loss': 1.6781, 'grad_norm': 4.353370189666748, 'learning_rate': 1.8433734939759036e-05, 'epoch': 2.35}\n",
            "{'loss': 1.703, 'grad_norm': 3.3356335163116455, 'learning_rate': 1.819277108433735e-05, 'epoch': 2.39}\n",
            "{'loss': 1.6673, 'grad_norm': 6.414052486419678, 'learning_rate': 1.7951807228915664e-05, 'epoch': 2.42}\n",
            "{'loss': 1.6997, 'grad_norm': 4.5114336013793945, 'learning_rate': 1.7710843373493978e-05, 'epoch': 2.46}\n",
            "{'loss': 1.6433, 'grad_norm': 4.484728813171387, 'learning_rate': 1.746987951807229e-05, 'epoch': 2.49}\n",
            "{'loss': 1.6409, 'grad_norm': 5.288791179656982, 'learning_rate': 1.7228915662650603e-05, 'epoch': 2.53}\n",
            "{'loss': 1.6691, 'grad_norm': 6.9035539627075195, 'learning_rate': 1.6987951807228917e-05, 'epoch': 2.56}\n",
            "{'loss': 1.6565, 'grad_norm': 6.246342182159424, 'learning_rate': 1.6746987951807228e-05, 'epoch': 2.6}\n",
            "{'loss': 1.6973, 'grad_norm': 4.27125883102417, 'learning_rate': 1.6506024096385542e-05, 'epoch': 2.63}\n",
            "{'loss': 1.6911, 'grad_norm': 4.582891941070557, 'learning_rate': 1.6265060240963853e-05, 'epoch': 2.66}\n",
            "{'loss': 1.6185, 'grad_norm': 8.956094741821289, 'learning_rate': 1.6024096385542168e-05, 'epoch': 2.7}\n",
            "{'loss': 1.6445, 'grad_norm': 3.8185203075408936, 'learning_rate': 1.5783132530120482e-05, 'epoch': 2.73}\n",
            "{'loss': 1.7063, 'grad_norm': 4.887276649475098, 'learning_rate': 1.5542168674698796e-05, 'epoch': 2.77}\n",
            " 55% 800/1445 [19:08<07:58,  1.35it/s]***** Running Evaluation *****\n",
            "  Num examples = 287\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/36 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/36 [00:02<00:38,  1.13s/it]\u001b[A\n",
            "  8% 3/36 [00:03<00:38,  1.18s/it]\u001b[A\n",
            " 11% 4/36 [00:06<01:00,  1.90s/it]\u001b[A\n",
            " 14% 5/36 [00:08<01:01,  2.00s/it]\u001b[A\n",
            " 17% 6/36 [00:10<01:01,  2.06s/it]\u001b[A\n",
            " 19% 7/36 [00:14<01:11,  2.48s/it]\u001b[A\n",
            " 22% 8/36 [00:17<01:12,  2.58s/it]\u001b[A\n",
            " 25% 9/36 [00:19<01:05,  2.43s/it]\u001b[A\n",
            " 28% 10/36 [00:21<01:01,  2.36s/it]\u001b[A\n",
            " 31% 11/36 [00:24<01:02,  2.50s/it]\u001b[A\n",
            " 33% 12/36 [00:26<00:59,  2.49s/it]\u001b[A\n",
            " 36% 13/36 [00:32<01:22,  3.58s/it]\u001b[A\n",
            " 39% 14/36 [00:36<01:20,  3.65s/it]\u001b[A\n",
            " 42% 15/36 [00:40<01:18,  3.72s/it]\u001b[A\n",
            " 44% 16/36 [00:45<01:24,  4.24s/it]\u001b[A\n",
            " 47% 17/36 [00:50<01:23,  4.42s/it]\u001b[A\n",
            " 50% 18/36 [00:51<00:58,  3.23s/it]\u001b[A\n",
            " 53% 19/36 [00:51<00:41,  2.46s/it]\u001b[A\n",
            " 56% 20/36 [00:52<00:31,  1.98s/it]\u001b[A\n",
            " 58% 21/36 [00:53<00:23,  1.58s/it]\u001b[A\n",
            " 61% 22/36 [00:54<00:18,  1.36s/it]\u001b[A\n",
            " 64% 23/36 [00:58<00:28,  2.17s/it]\u001b[A\n",
            " 67% 24/36 [01:00<00:25,  2.16s/it]\u001b[A\n",
            " 69% 25/36 [01:06<00:37,  3.44s/it]\u001b[A\n",
            " 72% 26/36 [01:09<00:32,  3.22s/it]\u001b[A\n",
            " 75% 27/36 [01:11<00:26,  2.93s/it]\u001b[A\n",
            " 78% 28/36 [01:13<00:21,  2.69s/it]\u001b[A\n",
            " 81% 29/36 [01:17<00:19,  2.82s/it]\u001b[A\n",
            " 83% 30/36 [01:19<00:15,  2.65s/it]\u001b[A\n",
            " 86% 31/36 [01:24<00:16,  3.32s/it]\u001b[A\n",
            " 89% 32/36 [01:26<00:11,  2.89s/it]\u001b[A\n",
            " 92% 33/36 [01:29<00:08,  2.91s/it]\u001b[A\n",
            " 94% 34/36 [01:36<00:08,  4.35s/it]\u001b[A\n",
            " 97% 35/36 [01:38<00:03,  3.61s/it]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.7329221963882446, 'eval_denotation_accuracy': 0.43902439024390244, 'eval_runtime': 109.8137, 'eval_samples_per_second': 2.614, 'eval_steps_per_second': 0.328, 'epoch': 2.77}\n",
            " 55% 800/1445 [20:58<07:58,  1.35it/s]\n",
            "100% 36/36 [01:48<00:00,  2.83s/it]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-800\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 1024, 'num_beams': 4, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Configuration saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-800/config.json\n",
            "Configuration saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-800/generation_config.json\n",
            "Model weights saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-800/model.safetensors\n",
            "tokenizer config file saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-800/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-800/special_tokens_map.json\n",
            "{'loss': 1.6197, 'grad_norm': 6.718677520751953, 'learning_rate': 1.530120481927711e-05, 'epoch': 2.8}\n",
            "{'loss': 1.6529, 'grad_norm': 2.281524658203125, 'learning_rate': 1.5060240963855423e-05, 'epoch': 2.84}\n",
            "{'loss': 1.7075, 'grad_norm': 4.238497734069824, 'learning_rate': 1.4819277108433735e-05, 'epoch': 2.87}\n",
            "{'loss': 1.6021, 'grad_norm': 4.328855991363525, 'learning_rate': 1.4578313253012048e-05, 'epoch': 2.91}\n",
            "{'loss': 1.652, 'grad_norm': 2.182048797607422, 'learning_rate': 1.4337349397590362e-05, 'epoch': 2.94}\n",
            "{'loss': 1.6412, 'grad_norm': 5.603063583374023, 'learning_rate': 1.4096385542168676e-05, 'epoch': 2.98}\n",
            "{'loss': 1.7226, 'grad_norm': 3.9690909385681152, 'learning_rate': 1.3855421686746989e-05, 'epoch': 3.01}\n",
            "{'loss': 1.5539, 'grad_norm': 3.136427402496338, 'learning_rate': 1.3614457831325301e-05, 'epoch': 3.04}\n",
            "{'loss': 1.6415, 'grad_norm': 1.8599485158920288, 'learning_rate': 1.3373493975903615e-05, 'epoch': 3.08}\n",
            "{'loss': 1.623, 'grad_norm': 2.983039617538452, 'learning_rate': 1.3132530120481928e-05, 'epoch': 3.11}\n",
            "{'loss': 1.618, 'grad_norm': 4.567707061767578, 'learning_rate': 1.2891566265060242e-05, 'epoch': 3.15}\n",
            "{'loss': 1.6147, 'grad_norm': 4.257735252380371, 'learning_rate': 1.2650602409638555e-05, 'epoch': 3.18}\n",
            "{'loss': 1.6033, 'grad_norm': 3.753445625305176, 'learning_rate': 1.2409638554216867e-05, 'epoch': 3.22}\n",
            "{'loss': 1.6071, 'grad_norm': 3.3758468627929688, 'learning_rate': 1.2168674698795181e-05, 'epoch': 3.25}\n",
            "{'loss': 1.622, 'grad_norm': 6.256833076477051, 'learning_rate': 1.1927710843373494e-05, 'epoch': 3.29}\n",
            "{'loss': 1.5729, 'grad_norm': 4.0220208168029785, 'learning_rate': 1.1686746987951808e-05, 'epoch': 3.32}\n",
            "{'loss': 1.6843, 'grad_norm': 1.8570668697357178, 'learning_rate': 1.1445783132530122e-05, 'epoch': 3.36}\n",
            "{'loss': 1.6215, 'grad_norm': 3.0829296112060547, 'learning_rate': 1.1204819277108433e-05, 'epoch': 3.39}\n",
            "{'loss': 1.5818, 'grad_norm': 1.5877206325531006, 'learning_rate': 1.0963855421686747e-05, 'epoch': 3.43}\n",
            "{'loss': 1.6127, 'grad_norm': 3.130342721939087, 'learning_rate': 1.072289156626506e-05, 'epoch': 3.46}\n",
            " 69% 1000/1445 [23:36<05:11,  1.43it/s]***** Running Evaluation *****\n",
            "  Num examples = 287\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/36 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/36 [00:02<00:38,  1.13s/it]\u001b[A\n",
            "  8% 3/36 [00:03<00:36,  1.10s/it]\u001b[A\n",
            " 11% 4/36 [00:07<01:07,  2.11s/it]\u001b[A\n",
            " 14% 5/36 [00:09<01:13,  2.38s/it]\u001b[A\n",
            " 17% 6/36 [00:13<01:26,  2.87s/it]\u001b[A\n",
            " 19% 7/36 [00:17<01:32,  3.19s/it]\u001b[A\n",
            " 22% 8/36 [00:21<01:30,  3.22s/it]\u001b[A\n",
            " 25% 9/36 [00:23<01:16,  2.85s/it]\u001b[A\n",
            " 28% 10/36 [00:25<01:08,  2.64s/it]\u001b[A\n",
            " 31% 11/36 [00:33<01:46,  4.26s/it]\u001b[A\n",
            " 33% 12/36 [00:35<01:29,  3.73s/it]\u001b[A\n",
            " 36% 13/36 [00:42<01:48,  4.71s/it]\u001b[A\n",
            " 39% 14/36 [00:46<01:39,  4.51s/it]\u001b[A\n",
            " 42% 15/36 [00:50<01:32,  4.39s/it]\u001b[A\n",
            " 44% 16/36 [00:57<01:38,  4.94s/it]\u001b[A\n",
            " 47% 17/36 [01:02<01:35,  5.01s/it]\u001b[A\n",
            " 50% 18/36 [01:02<01:05,  3.64s/it]\u001b[A\n",
            " 53% 19/36 [01:03<00:46,  2.74s/it]\u001b[A\n",
            " 56% 20/36 [01:04<00:34,  2.16s/it]\u001b[A\n",
            " 58% 21/36 [01:04<00:25,  1.71s/it]\u001b[A\n",
            " 61% 22/36 [01:05<00:20,  1.44s/it]\u001b[A\n",
            " 64% 23/36 [01:08<00:23,  1.80s/it]\u001b[A\n",
            " 67% 24/36 [01:10<00:23,  1.94s/it]\u001b[A\n",
            " 69% 25/36 [01:17<00:39,  3.55s/it]\u001b[A\n",
            " 72% 26/36 [01:20<00:34,  3.43s/it]\u001b[A\n",
            " 75% 27/36 [01:23<00:28,  3.12s/it]\u001b[A\n",
            " 78% 28/36 [01:25<00:22,  2.87s/it]\u001b[A\n",
            " 81% 29/36 [01:29<00:21,  3.12s/it]\u001b[A\n",
            " 83% 30/36 [01:31<00:16,  2.80s/it]\u001b[A\n",
            " 86% 31/36 [01:38<00:20,  4.15s/it]\u001b[A\n",
            " 89% 32/36 [01:40<00:14,  3.55s/it]\u001b[A\n",
            " 92% 33/36 [01:43<00:10,  3.36s/it]\u001b[A\n",
            " 94% 34/36 [01:53<00:10,  5.17s/it]\u001b[A\n",
            " 97% 35/36 [01:55<00:04,  4.47s/it]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.6989779472351074, 'eval_denotation_accuracy': 0.5156794425087108, 'eval_runtime': 125.0662, 'eval_samples_per_second': 2.295, 'eval_steps_per_second': 0.288, 'epoch': 3.46}\n",
            " 69% 1000/1445 [25:41<05:11,  1.43it/s]\n",
            "100% 36/36 [02:04<00:00,  3.52s/it]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-1000\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 1024, 'num_beams': 4, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Configuration saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-1000/config.json\n",
            "Configuration saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-1000/generation_config.json\n",
            "Model weights saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-1000/model.safetensors\n",
            "tokenizer config file saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 1.5905, 'grad_norm': 4.0258989334106445, 'learning_rate': 1.0481927710843374e-05, 'epoch': 3.49}\n",
            "{'loss': 1.6272, 'grad_norm': 4.195716381072998, 'learning_rate': 1.0240963855421688e-05, 'epoch': 3.53}\n",
            "{'loss': 1.6048, 'grad_norm': 2.3341448307037354, 'learning_rate': 9.999999999999999e-06, 'epoch': 3.56}\n",
            "{'loss': 1.5878, 'grad_norm': 3.3080332279205322, 'learning_rate': 9.759036144578313e-06, 'epoch': 3.6}\n",
            "{'loss': 1.6281, 'grad_norm': 2.976473331451416, 'learning_rate': 9.518072289156628e-06, 'epoch': 3.63}\n",
            "{'loss': 1.6014, 'grad_norm': 3.2561583518981934, 'learning_rate': 9.27710843373494e-06, 'epoch': 3.67}\n",
            "{'loss': 1.5966, 'grad_norm': 4.066337585449219, 'learning_rate': 9.036144578313254e-06, 'epoch': 3.7}\n",
            "{'loss': 1.6067, 'grad_norm': 2.2926764488220215, 'learning_rate': 8.795180722891565e-06, 'epoch': 3.74}\n",
            "{'loss': 1.6026, 'grad_norm': 3.508849620819092, 'learning_rate': 8.55421686746988e-06, 'epoch': 3.77}\n",
            "{'loss': 1.5698, 'grad_norm': 4.284769535064697, 'learning_rate': 8.313253012048194e-06, 'epoch': 3.81}\n",
            "{'loss': 1.5528, 'grad_norm': 2.0806663036346436, 'learning_rate': 8.072289156626506e-06, 'epoch': 3.84}\n",
            "{'loss': 1.6459, 'grad_norm': 3.440535306930542, 'learning_rate': 7.83132530120482e-06, 'epoch': 3.88}\n",
            "{'loss': 1.6436, 'grad_norm': 2.949580669403076, 'learning_rate': 7.590361445783132e-06, 'epoch': 3.91}\n",
            "{'loss': 1.5442, 'grad_norm': 3.1165382862091064, 'learning_rate': 7.349397590361446e-06, 'epoch': 3.94}\n",
            "{'loss': 1.5813, 'grad_norm': 11.23037338256836, 'learning_rate': 7.1084337349397595e-06, 'epoch': 3.98}\n",
            "{'loss': 1.5988, 'grad_norm': 6.4973297119140625, 'learning_rate': 6.867469879518072e-06, 'epoch': 4.01}\n",
            "{'loss': 1.608, 'grad_norm': 5.229145526885986, 'learning_rate': 6.626506024096385e-06, 'epoch': 4.05}\n",
            "{'loss': 1.6076, 'grad_norm': 5.988895416259766, 'learning_rate': 6.3855421686747e-06, 'epoch': 4.08}\n",
            "{'loss': 1.5403, 'grad_norm': 2.4844164848327637, 'learning_rate': 6.144578313253012e-06, 'epoch': 4.12}\n",
            "{'loss': 1.572, 'grad_norm': 4.296273231506348, 'learning_rate': 5.9036144578313255e-06, 'epoch': 4.15}\n",
            " 83% 1200/1445 [28:19<03:16,  1.25it/s]***** Running Evaluation *****\n",
            "  Num examples = 287\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/36 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/36 [00:02<00:36,  1.07s/it]\u001b[A\n",
            "  8% 3/36 [00:03<00:35,  1.06s/it]\u001b[A\n",
            " 11% 4/36 [00:06<00:58,  1.82s/it]\u001b[A\n",
            " 14% 5/36 [00:08<01:01,  2.00s/it]\u001b[A\n",
            " 17% 6/36 [00:11<01:07,  2.25s/it]\u001b[A\n",
            " 19% 7/36 [00:14<01:17,  2.67s/it]\u001b[A\n",
            " 22% 8/36 [00:17<01:15,  2.69s/it]\u001b[A\n",
            " 25% 9/36 [00:19<01:04,  2.40s/it]\u001b[A\n",
            " 28% 10/36 [00:21<00:59,  2.29s/it]\u001b[A\n",
            " 31% 11/36 [00:24<01:02,  2.49s/it]\u001b[A\n",
            " 33% 12/36 [00:27<01:00,  2.54s/it]\u001b[A\n",
            " 36% 13/36 [00:33<01:24,  3.66s/it]\u001b[A\n",
            " 39% 14/36 [00:37<01:21,  3.72s/it]\u001b[A\n",
            " 42% 15/36 [00:41<01:20,  3.84s/it]\u001b[A\n",
            " 44% 16/36 [00:46<01:25,  4.29s/it]\u001b[A\n",
            " 47% 17/36 [00:51<01:24,  4.44s/it]\u001b[A\n",
            " 50% 18/36 [00:51<00:58,  3.25s/it]\u001b[A\n",
            " 53% 19/36 [00:52<00:42,  2.47s/it]\u001b[A\n",
            " 56% 20/36 [00:53<00:31,  1.99s/it]\u001b[A\n",
            " 58% 21/36 [00:54<00:23,  1.60s/it]\u001b[A\n",
            " 61% 22/36 [00:54<00:19,  1.36s/it]\u001b[A\n",
            " 64% 23/36 [00:57<00:20,  1.58s/it]\u001b[A\n",
            " 67% 24/36 [00:59<00:20,  1.70s/it]\u001b[A\n",
            " 69% 25/36 [01:05<00:34,  3.18s/it]\u001b[A\n",
            " 72% 26/36 [01:08<00:29,  2.94s/it]\u001b[A\n",
            " 75% 27/36 [01:10<00:25,  2.82s/it]\u001b[A\n",
            " 78% 28/36 [01:12<00:20,  2.59s/it]\u001b[A\n",
            " 81% 29/36 [01:15<00:19,  2.72s/it]\u001b[A\n",
            " 83% 30/36 [01:17<00:14,  2.46s/it]\u001b[A\n",
            " 86% 31/36 [01:20<00:13,  2.65s/it]\u001b[A\n",
            " 89% 32/36 [01:22<00:09,  2.47s/it]\u001b[A\n",
            " 92% 33/36 [01:25<00:07,  2.49s/it]\u001b[A\n",
            " 94% 34/36 [01:32<00:07,  3.98s/it]\u001b[A\n",
            " 97% 35/36 [01:34<00:03,  3.34s/it]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.674998164176941, 'eval_denotation_accuracy': 0.5400696864111498, 'eval_runtime': 104.998, 'eval_samples_per_second': 2.733, 'eval_steps_per_second': 0.343, 'epoch': 4.15}\n",
            " 83% 1200/1445 [30:04<03:16,  1.25it/s]\n",
            "100% 36/36 [01:44<00:00,  2.70s/it]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-1200\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 1024, 'num_beams': 4, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Configuration saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-1200/config.json\n",
            "Configuration saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-1200/generation_config.json\n",
            "Model weights saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-1200/model.safetensors\n",
            "tokenizer config file saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-1200/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-1200/special_tokens_map.json\n",
            "{'loss': 1.5486, 'grad_norm': 2.953489303588867, 'learning_rate': 5.662650602409638e-06, 'epoch': 4.19}\n",
            "{'loss': 1.578, 'grad_norm': 2.9481208324432373, 'learning_rate': 5.421686746987952e-06, 'epoch': 4.22}\n",
            "{'loss': 1.5406, 'grad_norm': 2.8422579765319824, 'learning_rate': 5.180722891566266e-06, 'epoch': 4.26}\n",
            "{'loss': 1.5426, 'grad_norm': 0.9171605110168457, 'learning_rate': 4.939759036144578e-06, 'epoch': 4.29}\n",
            "{'loss': 1.5971, 'grad_norm': 6.026061534881592, 'learning_rate': 4.6987951807228915e-06, 'epoch': 4.33}\n",
            "{'loss': 1.5936, 'grad_norm': 2.340230941772461, 'learning_rate': 4.457831325301205e-06, 'epoch': 4.36}\n",
            "{'loss': 1.6005, 'grad_norm': 4.121411323547363, 'learning_rate': 4.216867469879518e-06, 'epoch': 4.39}\n",
            "{'loss': 1.6097, 'grad_norm': 3.7498388290405273, 'learning_rate': 3.975903614457832e-06, 'epoch': 4.43}\n",
            "{'loss': 1.5255, 'grad_norm': 4.166971206665039, 'learning_rate': 3.7349397590361445e-06, 'epoch': 4.46}\n",
            "{'loss': 1.5931, 'grad_norm': 2.163151741027832, 'learning_rate': 3.493975903614458e-06, 'epoch': 4.5}\n",
            "{'loss': 1.5781, 'grad_norm': 3.0833325386047363, 'learning_rate': 3.2530120481927713e-06, 'epoch': 4.53}\n",
            "{'loss': 1.5382, 'grad_norm': 1.4496500492095947, 'learning_rate': 3.012048192771084e-06, 'epoch': 4.57}\n",
            "{'loss': 1.5446, 'grad_norm': 2.9935436248779297, 'learning_rate': 2.7710843373493976e-06, 'epoch': 4.6}\n",
            "{'loss': 1.5722, 'grad_norm': 1.6172776222229004, 'learning_rate': 2.530120481927711e-06, 'epoch': 4.64}\n",
            "{'loss': 1.5534, 'grad_norm': 4.198538303375244, 'learning_rate': 2.2891566265060243e-06, 'epoch': 4.67}\n",
            "{'loss': 1.5447, 'grad_norm': 5.449224472045898, 'learning_rate': 2.0481927710843377e-06, 'epoch': 4.71}\n",
            "{'loss': 1.5432, 'grad_norm': 2.722860097885132, 'learning_rate': 1.8072289156626506e-06, 'epoch': 4.74}\n",
            "{'loss': 1.578, 'grad_norm': 1.9193356037139893, 'learning_rate': 1.5662650602409638e-06, 'epoch': 4.78}\n",
            "{'loss': 1.5874, 'grad_norm': 3.2832133769989014, 'learning_rate': 1.3253012048192771e-06, 'epoch': 4.81}\n",
            "{'loss': 1.5705, 'grad_norm': 5.259202003479004, 'learning_rate': 1.0843373493975903e-06, 'epoch': 4.84}\n",
            " 97% 1400/1445 [32:42<00:31,  1.41it/s]***** Running Evaluation *****\n",
            "  Num examples = 287\n",
            "  Batch size = 8\n",
            "\n",
            "  0% 0/36 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/36 [00:02<00:36,  1.08s/it]\u001b[A\n",
            "  8% 3/36 [00:03<00:35,  1.07s/it]\u001b[A\n",
            " 11% 4/36 [00:06<01:01,  1.93s/it]\u001b[A\n",
            " 14% 5/36 [00:09<01:07,  2.16s/it]\u001b[A\n",
            " 17% 6/36 [00:12<01:17,  2.57s/it]\u001b[A\n",
            " 19% 7/36 [00:16<01:25,  2.95s/it]\u001b[A\n",
            " 22% 8/36 [00:19<01:21,  2.91s/it]\u001b[A\n",
            " 25% 9/36 [00:20<01:08,  2.55s/it]\u001b[A\n",
            " 28% 10/36 [00:22<01:02,  2.39s/it]\u001b[A\n",
            " 31% 11/36 [00:26<01:07,  2.69s/it]\u001b[A\n",
            " 33% 12/36 [00:28<01:04,  2.67s/it]\u001b[A\n",
            " 36% 13/36 [00:35<01:28,  3.84s/it]\u001b[A\n",
            " 39% 14/36 [00:39<01:24,  3.86s/it]\u001b[A\n",
            " 42% 15/36 [00:43<01:22,  3.93s/it]\u001b[A\n",
            " 44% 16/36 [00:49<01:29,  4.47s/it]\u001b[A\n",
            " 47% 17/36 [00:54<01:29,  4.73s/it]\u001b[A\n",
            " 50% 18/36 [00:55<01:02,  3.45s/it]\u001b[A\n",
            " 53% 19/36 [00:55<00:44,  2.61s/it]\u001b[A\n",
            " 56% 20/36 [00:56<00:33,  2.09s/it]\u001b[A\n",
            " 58% 21/36 [00:57<00:24,  1.66s/it]\u001b[A\n",
            " 61% 22/36 [00:57<00:19,  1.40s/it]\u001b[A\n",
            " 64% 23/36 [01:00<00:21,  1.67s/it]\u001b[A\n",
            " 67% 24/36 [01:02<00:21,  1.76s/it]\u001b[A\n",
            " 69% 25/36 [01:09<00:36,  3.28s/it]\u001b[A\n",
            " 72% 26/36 [01:11<00:30,  3.03s/it]\u001b[A\n",
            " 75% 27/36 [01:13<00:25,  2.84s/it]\u001b[A\n",
            " 78% 28/36 [01:15<00:20,  2.61s/it]\u001b[A\n",
            " 81% 29/36 [01:19<00:19,  2.82s/it]\u001b[A\n",
            " 83% 30/36 [01:21<00:15,  2.53s/it]\u001b[A\n",
            " 86% 31/36 [01:24<00:13,  2.75s/it]\u001b[A\n",
            " 89% 32/36 [01:26<00:10,  2.56s/it]\u001b[A\n",
            " 92% 33/36 [01:28<00:07,  2.52s/it]\u001b[A\n",
            " 94% 34/36 [01:39<00:09,  4.90s/it]\u001b[A\n",
            " 97% 35/36 [01:42<00:04,  4.28s/it]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.6713049411773682, 'eval_denotation_accuracy': 0.5644599303135889, 'eval_runtime': 110.9679, 'eval_samples_per_second': 2.586, 'eval_steps_per_second': 0.324, 'epoch': 4.84}\n",
            " 97% 1400/1445 [34:33<00:31,  1.41it/s]\n",
            "100% 36/36 [01:50<00:00,  3.32s/it]\u001b[A\n",
            "                                   \u001b[ASaving model checkpoint to /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-1400\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 1024, 'num_beams': 4, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Configuration saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-1400/config.json\n",
            "Configuration saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-1400/generation_config.json\n",
            "Model weights saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-1400/model.safetensors\n",
            "tokenizer config file saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-1400/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/checkpoint-1400/special_tokens_map.json\n",
            "{'loss': 1.5447, 'grad_norm': 3.289400815963745, 'learning_rate': 8.433734939759036e-07, 'epoch': 4.88}\n",
            "{'loss': 1.5468, 'grad_norm': 2.391835927963257, 'learning_rate': 6.024096385542169e-07, 'epoch': 4.91}\n",
            "{'loss': 1.5803, 'grad_norm': 4.617743015289307, 'learning_rate': 3.6144578313253016e-07, 'epoch': 4.95}\n",
            "{'loss': 1.5587, 'grad_norm': 3.6030285358428955, 'learning_rate': 1.2048192771084337e-07, 'epoch': 4.98}\n",
            "100% 1445/1445 [35:17<00:00,  1.77it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 2117.2505, 'train_samples_per_second': 5.446, 'train_steps_per_second': 0.682, 'train_loss': 1.8002091137183167, 'epoch': 5.0}\n",
            "100% 1445/1445 [35:17<00:00,  1.47s/it]\n",
            "Saving model checkpoint to /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 1024, 'num_beams': 4, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
            "Configuration saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/config.json\n",
            "Configuration saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/generation_config.json\n",
            "Model weights saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/model.safetensors\n",
            "tokenizer config file saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        5.0\n",
            "  total_flos               =  3839474GF\n",
            "  train_loss               =     1.8002\n",
            "  train_runtime            = 0:35:17.25\n",
            "  train_samples            =       2306\n",
            "  train_samples_per_second =      5.446\n",
            "  train_steps_per_second   =      0.682\n",
            "06/19/2024 12:18:04 - INFO - __main__ -   *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 287\n",
            "  Batch size = 8\n",
            "100% 36/36 [01:48<00:00,  3.00s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                    =        5.0\n",
            "  eval_denotation_accuracy =     0.5679\n",
            "  eval_loss                =      1.669\n",
            "  eval_runtime             = 0:01:49.27\n",
            "  eval_samples             =        287\n",
            "  eval_samples_per_second  =      2.626\n",
            "  eval_steps_per_second    =      0.329\n"
          ]
        }
      ],
      "source": [
        "!python my_script.py \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --output_dir /content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/ \\\n",
        "  --model_name_or_path microsoft/tapex-base \\\n",
        "  --overwrite_output_dir \\\n",
        "  --per_device_train_batch_size 8 \\\n",
        "  --gradient_accumulation_steps 1 \\\n",
        "  --per_device_eval_batch_size 8 \\\n",
        "  --learning_rate 3e-5 \\\n",
        "  --logging_steps 10 \\\n",
        "  --eval_steps 200 \\\n",
        "  --save_steps 200 \\\n",
        "  --warmup_steps 200 \\\n",
        "  --evaluation_strategy steps \\\n",
        "  --predict_with_generate \\\n",
        "  --num_beams 5 \\\n",
        "  --weight_decay 1e-2 \\\n",
        "  --label_smoothing_factor 0.1 \\\n",
        "  --num_train_epochs=5 \\\n",
        "  --train_file /content/drive/MyDrive/TAPEX_Ressources/QATCH_DATA_TAPEX/train_df_spider_100prct.json \\\n",
        "  --validation_file /content/drive/MyDrive/TAPEX_Ressources/QATCH_DATA_TAPEX/valid_df_spider_10prct.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "N8cDQw1GBIP_"
      },
      "outputs": [],
      "source": [
        "#@title model class\n",
        "##Modified code from qatch repo\n",
        "import torch\n",
        "import pandas as pd\n",
        "from collections import Counter, defaultdict\n",
        "import numpy as np\n",
        "from transformers import AutoConfig, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import warnings\n",
        "\n",
        "class MyModel:\n",
        "    def __init__(self, model_name_or_path, device):\n",
        "        config = AutoConfig.from_pretrained(model_name_or_path)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path, config=config)\n",
        "\n",
        "        self.config = config\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "\n",
        "    def process_input(self, table: pd.DataFrame,\n",
        "                          query: str,\n",
        "                          tbl_name: str):\n",
        "            if table.shape[0] * table.shape[1] > 1024:\n",
        "                return None\n",
        "\n",
        "            # convert table to string\n",
        "            table = table.astype(str)\n",
        "            # process table\n",
        "            for col in table.columns:\n",
        "                table[col] = table[col].str.lower()\n",
        "\n",
        "            # tapex accepts uncased input since it is pre-trained on the uncased corpus\n",
        "            query = query.lower()\n",
        "            try:\n",
        "                model_input = self.tokenizer(table=table, query=query,\n",
        "                                            padding=True, return_tensors=\"pt\")\n",
        "            except ValueError as e:\n",
        "                # we get error when the tokenized input is longer than accepted from model\n",
        "                logging.warning(e)\n",
        "                return None\n",
        "\n",
        "            if model_input.input_ids.shape[1] > 1024:\n",
        "                warnings.warn(f'After tokenization'\n",
        "                              f' the input is longer than 1024 tokens: '\n",
        "                              f'{model_input.input_ids.shape[1]}. '\n",
        "                              'the input will be skipped')\n",
        "                return None\n",
        "\n",
        "            return model_input.to(self.device)\n",
        "\n",
        "    def predict_input(self, model_input, table):\n",
        "#I added this ,max_new_tokens=1024 to spress a warning during inference on spiderDev\n",
        "            outputs = self.model.generate(**model_input,max_new_tokens=1024)\n",
        "            model_input.to('cpu')\n",
        "\n",
        "            [model_input[idx].detach() for idx in model_input]\n",
        "            outputs = outputs.detach().cpu()\n",
        "\n",
        "            # decode back to text\n",
        "            pred_cells_queries = self.tokenizer.batch_decode(outputs,\n",
        "                                                            skip_special_tokens=True)\n",
        "            # the output contains list of string for each query. Manually transform the output\n",
        "            answers = []\n",
        "            for pred_query in pred_cells_queries:\n",
        "                query_ans = self._return_cells_aggr_by_row(table, pred_query)\n",
        "                answers.extend(query_ans)\n",
        "            del model_input\n",
        "            del outputs\n",
        "            return answers\n",
        "\n",
        "    @staticmethod\n",
        "    def _return_cells_aggr_by_row(table, pred_query):\n",
        "            \"\"\"\n",
        "            Perform an aggregation operation by row of the cells in a specified table based on a predicate query.\n",
        "\n",
        "            Args:\n",
        "                table (np.array): The table to perform the operation on.\n",
        "                pred_query (str): The predicate query used for aggregation operation. It should be a string of comma\n",
        "                                  separated cell values, e.g., \"cell1,cell2,cell3\".\n",
        "\n",
        "            Returns:\n",
        "                list: Returns a list of lists where each sublist contains aggregated cell values from a single row of the table.\n",
        "\n",
        "            Example:\n",
        "                Let's assume we have a table as below:\n",
        "\n",
        "                [[\"cell1\", \"cell2\"],\n",
        "                [\"cell3\", \"cell1\"],\n",
        "                [\"cell1\", \"cell2\"]]\n",
        "\n",
        "                And pred_query as \"cell1,cell1,cell2\"\n",
        "\n",
        "                Calling _return_cells_aggr_by_row(table, pred_query) will give:\n",
        "\n",
        "                [[\"cell1\", \"cell2\"], [\"cell1\"], [\"cell1\", \"cell2\"]]\n",
        "\n",
        "            Note:\n",
        "                If a cell from the pred_query is not present in the table, the method treats it as if it's in an imaginary\n",
        "                row indexed as -1. Therefore, if you see a [-1] in the result, it means one or more cells in your pred_query\n",
        "                did not appear in the table.\n",
        "            \"\"\"\n",
        "            # Initializing a defaultdict to store the results of the query\n",
        "            query_ans = defaultdict(list)\n",
        "            # Splitting the query into cells\n",
        "            cells: list = pred_query.split(\",\")\n",
        "            # Counting the occurrences of each cell in the query\n",
        "            counted_cells = Counter(cells)\n",
        "            # Iterating over each cell type and its count from the counted_cells\n",
        "            for cell, count in counted_cells.items():\n",
        "                # Finding the row ids where the current cell type exists in the table\n",
        "                row_ids = np.where(table == cell.strip())[0]\n",
        "                if len(row_ids) == 0:\n",
        "                    # If the cell is not present in the table, set the row_id as -1\n",
        "                    row_ids = [-1]\n",
        "\n",
        "                # If the count of cell in the query > 1, select the first 'count' number of rows\n",
        "                if count > 1:\n",
        "                    row_ids = row_ids[:count]\n",
        "                # Appending the cell to the rows in the query_ans for each row_id\n",
        "                [query_ans[idx].append(cell.strip()) for idx in row_ids]\n",
        "            # Return the aggregated cells from each row as a list of lists\n",
        "            return list(query_ans.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3zCn0zgEUgt",
        "outputId": "41905b33-4021-4431-a9e2-a192c350bced"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BartForConditionalGeneration(\n",
              "  (model): BartModel(\n",
              "    (shared): Embedding(50265, 768, padding_idx=1)\n",
              "    (encoder): BartEncoder(\n",
              "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartEncoderLayer(\n",
              "          (self_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartDecoderLayer(\n",
              "          (self_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "# Load the model and tokenizer\n",
        "model_name_or_path = \"/content/drive/MyDrive/TAPEX_Ressources/checkpoints/train_df_spider_100prct_checkpoint/\"\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "my_model = MyModel(model_name_or_path, device)\n",
        "my_model.model.to(device)\n",
        "my_model.model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title tapas base no training\n",
        "# Load the model and tokenizer\n",
        "model_name_or_path = \"microsoft/tapex-base\"\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "my_model = MyModel(model_name_or_path, device)\n",
        "my_model.model.to(device)\n",
        "my_model.model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "yaokWpJVms3-",
        "outputId": "e93a1133-1eda-49ae-96ba-d337f24de200"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BartForConditionalGeneration(\n",
              "  (model): BartModel(\n",
              "    (shared): Embedding(50265, 768, padding_idx=1)\n",
              "    (encoder): BartEncoder(\n",
              "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartEncoderLayer(\n",
              "          (self_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartDecoderLayer(\n",
              "          (self_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MR_JOYdCPXBz"
      },
      "outputs": [],
      "source": [
        "def getAnswer(row):\n",
        "    row_index = row['ID']\n",
        "    table_used = row['table_used']\n",
        "    table = db_to_df[table_used]\n",
        "    table = table.astype(str)\n",
        "    query_nl = row[\"question\"]\n",
        "\n",
        "    # Preprocess the input data and make predictions\n",
        "    model_input = my_model.process_input(table, query_nl, table_used)\n",
        "    if model_input is not None:\n",
        "        answers = my_model.predict_input(model_input, table)\n",
        "        return answers\n",
        "    else:\n",
        "        print(f\"The input for row {row_index} was too long and was skipped.\")\n",
        "        return None\n",
        "\n",
        "def getAnswerSpiderDev(row):\n",
        "    row_index = row['ID']\n",
        "\n",
        "    seq_id = row[\"db_id\"] + \"_X_\" + row[\"table_used\"]\n",
        "    if seq_id not in db_to_df_spiderDev or db_to_df_spiderDev[seq_id].empty:\n",
        "        print(f\"Row {row_index} skipped: table not found or empty.\")\n",
        "        return None\n",
        "\n",
        "    table_used=row[\"table_used\"]\n",
        "    table = db_to_df_spiderDev[seq_id]\n",
        "    table = table.astype(str)\n",
        "    query_nl = row[\"question\"]\n",
        "    # Preprocess the input data and make predictions\n",
        "    model_input = my_model.process_input(table, query_nl, table_used)\n",
        "    if model_input is not None:\n",
        "        answers = my_model.predict_input(model_input, table)\n",
        "        return answers\n",
        "    else:\n",
        "        print(f\"The input for row {row_index} was too long and was skipped.\")\n",
        "        return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5EY1LzTeNInP"
      },
      "outputs": [],
      "source": [
        "#@title calculate_and_log_stats\n",
        "def calculate_and_log_stats(epoch,avg_loss,df,valid=True):\n",
        "    my_model.model.eval()\n",
        "    with torch.no_grad():\n",
        "      df_cp=df.copy()\n",
        "      df_cp['predictions_TAPAS'] = df_cp.apply(getAnswer, axis=1)\n",
        "      tests_df_results = evaluator.evaluate_with_df(df_cp,\n",
        "                                            prediction_col_name=f'predictions_TAPAS',\n",
        "                                            task=\"QA\")\n",
        "    my_model.model.train()\n",
        "\n",
        "    columns_to_average = [\n",
        "        'cell_precision_predictions_TAPAS',\n",
        "        'cell_recall_predictions_TAPAS',\n",
        "        'tuple_cardinality_predictions_TAPAS',\n",
        "        'tuple_constraint_predictions_TAPAS',\n",
        "        'tuple_order_predictions_TAPAS'\n",
        "    ]\n",
        "\n",
        "\n",
        "    general_stats = {\n",
        "        'epoch': epoch,\n",
        "        'avg_loss': avg_loss,\n",
        "        'general_avg_cell_precision': tests_df_results[columns_to_average[0]].mean(),\n",
        "        'general_avg_cell_recall': tests_df_results[columns_to_average[1]].mean(),\n",
        "        'general_avg_tuple_cardinality': tests_df_results[columns_to_average[2]].mean(),\n",
        "        'general_avg_tuple_constraint': tests_df_results[columns_to_average[3]].mean()\n",
        "    }\n",
        "\n",
        "    print(f\"General Model Performance Metrics for epoch:{epoch}\")\n",
        "    for stat_name, stat_value in general_stats.items():\n",
        "        if stat_name != 'epoch':  # We don't need to print the epoch again\n",
        "            print(f\"{stat_name}: {stat_value}\")\n",
        "    print(\"----------------------------\")\n",
        "\n",
        "    grouped_means = tests_df_results.groupby('sql_tags')[columns_to_average].mean()\n",
        "\n",
        "\n",
        "    for sql_tag, row in grouped_means.iterrows():\n",
        "        for col in columns_to_average:\n",
        "            key_name = f\"{sql_tag}_{col}\"\n",
        "            general_stats[key_name] = row[col]\n",
        "\n",
        "    print(\"Grouped Stats by SQL Tag:\")\n",
        "    for sql_tag, metrics in grouped_means.iterrows():\n",
        "        print(f\"SQL Tag: {sql_tag}\")\n",
        "        for col in columns_to_average:\n",
        "            print(f\"  {col}: {metrics[col]}\")\n",
        "        print(\"----------------------------\")\n",
        "    return general_stats\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title calculate_and_log_stats for Spider\n",
        "def calculate_and_log_stats(epoch,avg_loss,df,valid=True):\n",
        "    my_model.model.eval()\n",
        "    with torch.no_grad():\n",
        "      df_cp=df.copy()\n",
        "      df_cp['predictions_TAPAS'] = df_cp.apply(getAnswerSpiderDev, axis=1)\n",
        "      tests_df_results = evaluator.evaluate_with_df(df_cp,\n",
        "                                            prediction_col_name=f'predictions_TAPAS',\n",
        "                                            task=\"QA\")\n",
        "    my_model.model.train()\n",
        "\n",
        "    columns_to_average = [\n",
        "        'cell_precision_predictions_TAPAS',\n",
        "        'cell_recall_predictions_TAPAS',\n",
        "        'tuple_cardinality_predictions_TAPAS',\n",
        "        'tuple_constraint_predictions_TAPAS',\n",
        "        'tuple_order_predictions_TAPAS'\n",
        "    ]\n",
        "\n",
        "\n",
        "    general_stats = {\n",
        "        'epoch': epoch,\n",
        "        'avg_loss': avg_loss,\n",
        "        'general_avg_cell_precision': tests_df_results[columns_to_average[0]].mean(),\n",
        "        'general_avg_cell_recall': tests_df_results[columns_to_average[1]].mean(),\n",
        "        'general_avg_tuple_cardinality': tests_df_results[columns_to_average[2]].mean(),\n",
        "        'general_avg_tuple_constraint': tests_df_results[columns_to_average[3]].mean()\n",
        "    }\n",
        "\n",
        "    print(f\"General Model Performance Metrics for epoch:{epoch}\")\n",
        "    for stat_name, stat_value in general_stats.items():\n",
        "        if stat_name != 'epoch':  # We don't need to print the epoch again\n",
        "            print(f\"{stat_name}: {stat_value}\")\n",
        "    print(\"----------------------------\")\n",
        "\n",
        "    grouped_means = tests_df_results.groupby('sql_tags')[columns_to_average].mean()\n",
        "\n",
        "\n",
        "    for sql_tag, row in grouped_means.iterrows():\n",
        "        for col in columns_to_average:\n",
        "            key_name = f\"{sql_tag}_{col}\"\n",
        "            general_stats[key_name] = row[col]\n",
        "\n",
        "    print(\"Grouped Stats by SQL Tag:\")\n",
        "    for sql_tag, metrics in grouped_means.iterrows():\n",
        "        print(f\"SQL Tag: {sql_tag}\")\n",
        "        for col in columns_to_average:\n",
        "            print(f\"  {col}: {metrics[col]}\")\n",
        "        print(\"----------------------------\")\n",
        "    return general_stats\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "v7igyOsgqk1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOjNteAiULSl",
        "outputId": "fc9b3082-a643-4458-88b9-cc3460c00935"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sql_tags\n",
              "HAVING      15\n",
              "WHERE        8\n",
              "ORDERBY      3\n",
              "SELECT       3\n",
              "GROUPBY      2\n",
              "SIMPLE       2\n",
              "DISTINCT     1\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "train_df[\"sql_tags\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZy007oLUHwN",
        "outputId": "390ab3f7-b1fd-4249-d371-1e163da3560e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sql_tags\n",
              "HAVING      19\n",
              "WHERE        9\n",
              "SELECT       4\n",
              "GROUPBY      3\n",
              "ORDERBY      3\n",
              "SIMPLE       3\n",
              "DISTINCT     1\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "valid_df[\"sql_tags\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9hLZLY9T6tD",
        "outputId": "9ba7f163-187f-4b24-85a3-7a6f3f425237"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sql_tags\n",
              "HAVING      18\n",
              "WHERE       10\n",
              "SELECT       4\n",
              "GROUPBY      3\n",
              "ORDERBY      3\n",
              "SIMPLE       3\n",
              "DISTINCT     2\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "test_df[\"sql_tags\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxbZtbO2NW9b",
        "outputId": "62a0f17a-8d4f-4b99-9e6a-666b0fb5d431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n",
            "Evaluating QA tests: 100%|██████████| 42/42 [00:00<00:00, 245.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "General Model Performance Metrics for epoch:None\n",
            "avg_loss: None\n",
            "general_avg_cell_precision: 0.36904761904761907\n",
            "general_avg_cell_recall: 0.36904761904761907\n",
            "general_avg_tuple_cardinality: 0.24764285714285714\n",
            "general_avg_tuple_constraint: 0.11904761904761904\n",
            "----------------------------\n",
            "Grouped Stats by SQL Tag:\n",
            "SQL Tag: DISTINCT\n",
            "  cell_precision_predictions_TAPAS: 0.0\n",
            "  cell_recall_predictions_TAPAS: 0.0\n",
            "  tuple_cardinality_predictions_TAPAS: 0.0\n",
            "  tuple_constraint_predictions_TAPAS: 0.0\n",
            "  tuple_order_predictions_TAPAS: 0.0\n",
            "----------------------------\n",
            "SQL Tag: GROUPBY\n",
            "  cell_precision_predictions_TAPAS: 0.3333333333333333\n",
            "  cell_recall_predictions_TAPAS: 0.3333333333333333\n",
            "  tuple_cardinality_predictions_TAPAS: 0.3333333333333333\n",
            "  tuple_constraint_predictions_TAPAS: 0.0\n",
            "  tuple_order_predictions_TAPAS: 0.0\n",
            "----------------------------\n",
            "SQL Tag: HAVING\n",
            "  cell_precision_predictions_TAPAS: 0.7105263157894737\n",
            "  cell_recall_predictions_TAPAS: 0.7105263157894737\n",
            "  tuple_cardinality_predictions_TAPAS: 0.3930526315789474\n",
            "  tuple_constraint_predictions_TAPAS: 0.23684210526315788\n",
            "  tuple_order_predictions_TAPAS: nan\n",
            "----------------------------\n",
            "SQL Tag: ORDERBY\n",
            "  cell_precision_predictions_TAPAS: 0.3333333333333333\n",
            "  cell_recall_predictions_TAPAS: 0.3333333333333333\n",
            "  tuple_cardinality_predictions_TAPAS: 0.311\n",
            "  tuple_constraint_predictions_TAPAS: 0.16666666666666666\n",
            "  tuple_order_predictions_TAPAS: 0.3333333333333333\n",
            "----------------------------\n",
            "SQL Tag: SELECT\n",
            "  cell_precision_predictions_TAPAS: 0.0\n",
            "  cell_recall_predictions_TAPAS: 0.0\n",
            "  tuple_cardinality_predictions_TAPAS: 0.0\n",
            "  tuple_constraint_predictions_TAPAS: 0.0\n",
            "  tuple_order_predictions_TAPAS: 0.0\n",
            "----------------------------\n",
            "SQL Tag: SIMPLE\n",
            "  cell_precision_predictions_TAPAS: 0.0\n",
            "  cell_recall_predictions_TAPAS: 0.0\n",
            "  tuple_cardinality_predictions_TAPAS: 0.3333333333333333\n",
            "  tuple_constraint_predictions_TAPAS: 0.0\n",
            "  tuple_order_predictions_TAPAS: 0.0\n",
            "----------------------------\n",
            "SQL Tag: WHERE\n",
            "  cell_precision_predictions_TAPAS: 0.0\n",
            "  cell_recall_predictions_TAPAS: 0.0\n",
            "  tuple_cardinality_predictions_TAPAS: 0.0\n",
            "  tuple_constraint_predictions_TAPAS: 0.0\n",
            "  tuple_order_predictions_TAPAS: 0.0\n",
            "----------------------------\n"
          ]
        }
      ],
      "source": [
        "valid_df_stats=calculate_and_log_stats(None,None,valid_df,valid=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CwmZHf-Nbi-"
      },
      "outputs": [],
      "source": [
        "valid_stats_df = pd.DataFrame([valid_df_stats])\n",
        "valid_stats_df.to_csv('/content/drive/MyDrive/TAPEX_Ressources/Results/valid_stats_df_medicine_80prct.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_A3yJRyPNjXR",
        "outputId": "f45cf6dc-8569-4889-dc15-ffedc9107efc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating QA tests: 100%|██████████| 43/43 [00:00<00:00, 237.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "General Model Performance Metrics for epoch:None\n",
            "avg_loss: None\n",
            "general_avg_cell_precision: 0.4147209302325581\n",
            "general_avg_cell_recall: 0.408906976744186\n",
            "general_avg_tuple_cardinality: 0.22597674418604652\n",
            "general_avg_tuple_constraint: 0.06976744186046512\n",
            "----------------------------\n",
            "Grouped Stats by SQL Tag:\n",
            "SQL Tag: DISTINCT\n",
            "  cell_precision_predictions_TAPAS: 0.5\n",
            "  cell_recall_predictions_TAPAS: 0.5\n",
            "  tuple_cardinality_predictions_TAPAS: 0.0665\n",
            "  tuple_constraint_predictions_TAPAS: 0.0\n",
            "  tuple_order_predictions_TAPAS: 0.0\n",
            "----------------------------\n",
            "SQL Tag: GROUPBY\n",
            "  cell_precision_predictions_TAPAS: 0.3333333333333333\n",
            "  cell_recall_predictions_TAPAS: 0.25\n",
            "  tuple_cardinality_predictions_TAPAS: 0.22233333333333336\n",
            "  tuple_constraint_predictions_TAPAS: 0.0\n",
            "  tuple_order_predictions_TAPAS: 0.0\n",
            "----------------------------\n",
            "SQL Tag: HAVING\n",
            "  cell_precision_predictions_TAPAS: 0.75\n",
            "  cell_recall_predictions_TAPAS: 0.75\n",
            "  tuple_cardinality_predictions_TAPAS: 0.37683333333333335\n",
            "  tuple_constraint_predictions_TAPAS: 0.1111111111111111\n",
            "  tuple_order_predictions_TAPAS: nan\n",
            "----------------------------\n",
            "SQL Tag: ORDERBY\n",
            "  cell_precision_predictions_TAPAS: 0.4443333333333333\n",
            "  cell_recall_predictions_TAPAS: 0.4443333333333333\n",
            "  tuple_cardinality_predictions_TAPAS: 0.04466666666666667\n",
            "  tuple_constraint_predictions_TAPAS: 0.0\n",
            "  tuple_order_predictions_TAPAS: 0.3333333333333333\n",
            "----------------------------\n",
            "SQL Tag: SELECT\n",
            "  cell_precision_predictions_TAPAS: 0.0\n",
            "  cell_recall_predictions_TAPAS: 0.0\n",
            "  tuple_cardinality_predictions_TAPAS: 0.0\n",
            "  tuple_constraint_predictions_TAPAS: 0.0\n",
            "  tuple_order_predictions_TAPAS: 0.0\n",
            "----------------------------\n",
            "SQL Tag: SIMPLE\n",
            "  cell_precision_predictions_TAPAS: 0.3333333333333333\n",
            "  cell_recall_predictions_TAPAS: 0.3333333333333333\n",
            "  tuple_cardinality_predictions_TAPAS: 0.6666666666666666\n",
            "  tuple_constraint_predictions_TAPAS: 0.3333333333333333\n",
            "  tuple_order_predictions_TAPAS: 0.0\n",
            "----------------------------\n",
            "SQL Tag: WHERE\n",
            "  cell_precision_predictions_TAPAS: 0.0\n",
            "  cell_recall_predictions_TAPAS: 0.0\n",
            "  tuple_cardinality_predictions_TAPAS: 0.0\n",
            "  tuple_constraint_predictions_TAPAS: 0.0\n",
            "  tuple_order_predictions_TAPAS: 0.0\n",
            "----------------------------\n"
          ]
        }
      ],
      "source": [
        "test_df_stats=calculate_and_log_stats(None,None,test_df,valid=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uh0_3WKTNiDY"
      },
      "outputs": [],
      "source": [
        "test_stats_df = pd.DataFrame([test_df_stats])\n",
        "test_stats_df.to_csv('/content/drive/MyDrive/TAPEX_Ressources/Results/test_stats_df_medicine_80prct.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spider_test_stats=calculate_and_log_stats(None,None,spider_test_pd,valid=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-ykpqUGcgYa",
        "outputId": "5d2f05f3-4eb5-492f-c4de-ea4e2f7fe90a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1045 > 1024). Running this sequence through the model will result in indexing errors\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1045. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1043. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1044. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input for row 553 was too long and was skipped.\n",
            "The input for row 559 was too long and was skipped.\n",
            "The input for row 560 was too long and was skipped.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1034. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1035. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1036. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1039. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input for row 577 was too long and was skipped.\n",
            "The input for row 578 was too long and was skipped.\n",
            "The input for row 579 was too long and was skipped.\n",
            "The input for row 580 was too long and was skipped.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1142. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1143. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1086. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1085. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input for row 962 was too long and was skipped.\n",
            "The input for row 963 was too long and was skipped.\n",
            "The input for row 964 was too long and was skipped.\n",
            "The input for row 965 was too long and was skipped.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1099. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1104. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1082. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1081. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1081. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1081. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1135. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1135. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1084. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input for row 984 was too long and was skipped.\n",
            "The input for row 985 was too long and was skipped.\n",
            "The input for row 986 was too long and was skipped.\n",
            "The input for row 987 was too long and was skipped.\n",
            "The input for row 988 was too long and was skipped.\n",
            "The input for row 989 was too long and was skipped.\n",
            "The input for row 994 was too long and was skipped.\n",
            "The input for row 995 was too long and was skipped.\n",
            "The input for row 996 was too long and was skipped.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1084. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input for row 997 was too long and was skipped.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating QA tests:  42%|████▏     | 88/211 [00:02<00:03, 33.43it/s]WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "Evaluating QA tests:  51%|█████     | 107/211 [00:02<00:01, 53.59it/s]WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "Evaluating QA tests:  81%|████████  | 171/211 [00:03<00:00, 45.02it/s]WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "Evaluating QA tests:  90%|████████▉ | 189/211 [00:04<00:00, 44.90it/s]WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "Evaluating QA tests: 100%|██████████| 211/211 [00:04<00:00, 45.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "General Model Performance Metrics for epoch:None\n",
            "avg_loss: None\n",
            "general_avg_cell_precision: 0.5442322274881516\n",
            "general_avg_cell_recall: 0.5457345971563982\n",
            "general_avg_tuple_cardinality: 0.49359241706161117\n",
            "general_avg_tuple_constraint: 0.287521327014218\n",
            "----------------------------\n",
            "Grouped Stats by SQL Tag:\n",
            "SQL Tag: HAVING\n",
            "  cell_precision_predictions_TAPAS: 0.3349166666666667\n",
            "  cell_recall_predictions_TAPAS: 0.5625\n",
            "  tuple_cardinality_predictions_TAPAS: 0.45733333333333337\n",
            "  tuple_constraint_predictions_TAPAS: 0.38891666666666663\n",
            "  tuple_order_predictions_TAPAS: 0.0\n",
            "----------------------------\n",
            "SQL Tag: ORDERBY\n",
            "  cell_precision_predictions_TAPAS: 0.739361111111111\n",
            "  cell_recall_predictions_TAPAS: 0.6658055555555557\n",
            "  tuple_cardinality_predictions_TAPAS: 0.2010833333333334\n",
            "  tuple_constraint_predictions_TAPAS: 0.1111111111111111\n",
            "  tuple_order_predictions_TAPAS: 0.3763888888888889\n",
            "----------------------------\n",
            "SQL Tag: SELECT\n",
            "  cell_precision_predictions_TAPAS: 0.31372\n",
            "  cell_recall_predictions_TAPAS: 0.29868\n",
            "  tuple_cardinality_predictions_TAPAS: 0.18136\n",
            "  tuple_constraint_predictions_TAPAS: 0.16\n",
            "  tuple_order_predictions_TAPAS: 0.0\n",
            "----------------------------\n",
            "SQL Tag: SIMPLE_AGGR\n",
            "  cell_precision_predictions_TAPAS: 0.4827586206896552\n",
            "  cell_recall_predictions_TAPAS: 0.4827586206896552\n",
            "  tuple_cardinality_predictions_TAPAS: 0.6810172413793103\n",
            "  tuple_constraint_predictions_TAPAS: 0.4482758620689655\n",
            "  tuple_order_predictions_TAPAS: 0.0\n",
            "----------------------------\n",
            "SQL Tag: WHERE\n",
            "  cell_precision_predictions_TAPAS: 0.604425\n",
            "  cell_recall_predictions_TAPAS: 0.61205\n",
            "  tuple_cardinality_predictions_TAPAS: 0.59235\n",
            "  tuple_constraint_predictions_TAPAS: 0.275\n",
            "  tuple_order_predictions_TAPAS: 0.0\n",
            "----------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spider_test_stats=calculate_and_log_stats(None,None,spider_test_pd,valid=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWiPWYlUSv4J",
        "outputId": "f0cd2d08-61cd-4ed6-b41f-0d9855047495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1045 > 1024). Running this sequence through the model will result in indexing errors\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1045. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1043. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1044. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input for row 553 was too long and was skipped.\n",
            "The input for row 559 was too long and was skipped.\n",
            "The input for row 560 was too long and was skipped.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1034. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1035. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1036. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1039. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input for row 577 was too long and was skipped.\n",
            "The input for row 578 was too long and was skipped.\n",
            "The input for row 579 was too long and was skipped.\n",
            "The input for row 580 was too long and was skipped.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1142. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1143. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1086. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1085. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input for row 962 was too long and was skipped.\n",
            "The input for row 963 was too long and was skipped.\n",
            "The input for row 964 was too long and was skipped.\n",
            "The input for row 965 was too long and was skipped.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1099. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1104. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1082. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1081. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1081. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1081. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input for row 984 was too long and was skipped.\n",
            "The input for row 985 was too long and was skipped.\n",
            "The input for row 986 was too long and was skipped.\n",
            "The input for row 987 was too long and was skipped.\n",
            "The input for row 988 was too long and was skipped.\n",
            "The input for row 989 was too long and was skipped.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1135. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1135. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1084. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1084. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input for row 994 was too long and was skipped.\n",
            "The input for row 995 was too long and was skipped.\n",
            "The input for row 996 was too long and was skipped.\n",
            "The input for row 997 was too long and was skipped.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating QA tests:  48%|████▊     | 101/211 [00:03<00:02, 40.42it/s]WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "Evaluating QA tests:  80%|████████  | 169/211 [00:04<00:01, 37.34it/s]WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "Evaluating QA tests:  88%|████████▊ | 186/211 [00:04<00:00, 52.37it/s]WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "Evaluating QA tests:  93%|█████████▎| 196/211 [00:05<00:00, 43.71it/s]WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "Evaluating QA tests: 100%|██████████| 211/211 [00:05<00:00, 40.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "General Model Performance Metrics for epoch:None\n",
            "avg_loss: None\n",
            "general_avg_cell_precision: 0.5486303317535545\n",
            "general_avg_cell_recall: 0.552872037914692\n",
            "general_avg_tuple_cardinality: 0.5028957345971562\n",
            "general_avg_tuple_constraint: 0.29120379146919434\n",
            "----------------------------\n",
            "Grouped Stats by SQL Tag:\n",
            "SQL Tag: HAVING\n",
            "  cell_precision_predictions_TAPAS: 0.3205\n",
            "  cell_recall_predictions_TAPAS: 0.48150000000000004\n",
            "  tuple_cardinality_predictions_TAPAS: 0.458\n",
            "  tuple_constraint_predictions_TAPAS: 0.287\n",
            "  tuple_order_predictions_TAPAS: 0.0\n",
            "----------------------------\n",
            "SQL Tag: ORDERBY\n",
            "  cell_precision_predictions_TAPAS: 0.714111111111111\n",
            "  cell_recall_predictions_TAPAS: 0.6596388888888891\n",
            "  tuple_cardinality_predictions_TAPAS: 0.19800000000000006\n",
            "  tuple_constraint_predictions_TAPAS: 0.1111111111111111\n",
            "  tuple_order_predictions_TAPAS: 0.3819444444444444\n",
            "----------------------------\n",
            "SQL Tag: SELECT\n",
            "  cell_precision_predictions_TAPAS: 0.23751999999999998\n",
            "  cell_recall_predictions_TAPAS: 0.22\n",
            "  tuple_cardinality_predictions_TAPAS: 0.13068000000000002\n",
            "  tuple_constraint_predictions_TAPAS: 0.08\n",
            "  tuple_order_predictions_TAPAS: 0.0\n",
            "----------------------------\n",
            "SQL Tag: SIMPLE_AGGR\n",
            "  cell_precision_predictions_TAPAS: 0.4827586206896552\n",
            "  cell_recall_predictions_TAPAS: 0.4827586206896552\n",
            "  tuple_cardinality_predictions_TAPAS: 0.7183793103448275\n",
            "  tuple_constraint_predictions_TAPAS: 0.4482758620689655\n",
            "  tuple_order_predictions_TAPAS: 0.0\n",
            "----------------------------\n",
            "SQL Tag: WHERE\n",
            "  cell_precision_predictions_TAPAS: 0.6533625\n",
            "  cell_recall_predictions_TAPAS: 0.6703875\n",
            "  tuple_cardinality_predictions_TAPAS: 0.606925\n",
            "  tuple_constraint_predictions_TAPAS: 0.325\n",
            "  tuple_order_predictions_TAPAS: 0.0\n",
            "----------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spider_test_stats=calculate_and_log_stats(None,None,spider_test_pd,valid=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLXAMmepAxrX",
        "outputId": "56e9ff02-5998-4adc-d08f-4c82da6e3904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1045 > 1024). Running this sequence through the model will result in indexing errors\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1045. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1043. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1044. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input for row 553 was too long and was skipped.\n",
            "The input for row 559 was too long and was skipped.\n",
            "The input for row 560 was too long and was skipped.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1034. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1035. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1036. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1039. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input for row 577 was too long and was skipped.\n",
            "The input for row 578 was too long and was skipped.\n",
            "The input for row 579 was too long and was skipped.\n",
            "The input for row 580 was too long and was skipped.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1142. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1143. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1086. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1085. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input for row 962 was too long and was skipped.\n",
            "The input for row 963 was too long and was skipped.\n",
            "The input for row 964 was too long and was skipped.\n",
            "The input for row 965 was too long and was skipped.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1099. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1104. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1082. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1081. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1081. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1081. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1135. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1135. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1084. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input for row 984 was too long and was skipped.\n",
            "The input for row 985 was too long and was skipped.\n",
            "The input for row 986 was too long and was skipped.\n",
            "The input for row 987 was too long and was skipped.\n",
            "The input for row 988 was too long and was skipped.\n",
            "The input for row 989 was too long and was skipped.\n",
            "The input for row 994 was too long and was skipped.\n",
            "The input for row 995 was too long and was skipped.\n",
            "The input for row 996 was too long and was skipped.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1084. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input for row 997 was too long and was skipped.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating QA tests:  44%|████▍     | 93/211 [00:00<00:00, 220.20it/s]WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "Evaluating QA tests:  87%|████████▋ | 184/211 [00:00<00:00, 198.84it/s]WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "Evaluating QA tests: 100%|██████████| 211/211 [00:00<00:00, 213.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "General Model Performance Metrics for epoch:None\n",
            "avg_loss: None\n",
            "general_avg_cell_precision: 0.5502417061611373\n",
            "general_avg_cell_recall: 0.5545355450236967\n",
            "general_avg_tuple_cardinality: 0.48390995260663494\n",
            "general_avg_tuple_constraint: 0.2906777251184834\n",
            "----------------------------\n",
            "Grouped Stats by SQL Tag:\n",
            "SQL Tag: HAVING\n",
            "  cell_precision_predictions_TAPAS: 0.3114166666666667\n",
            "  cell_recall_predictions_TAPAS: 0.48608333333333337\n",
            "  tuple_cardinality_predictions_TAPAS: 0.48808333333333337\n",
            "  tuple_constraint_predictions_TAPAS: 0.27775\n",
            "  tuple_order_predictions_TAPAS: 0.0\n",
            "----------------------------\n",
            "SQL Tag: ORDERBY\n",
            "  cell_precision_predictions_TAPAS: 0.7129444444444444\n",
            "  cell_recall_predictions_TAPAS: 0.6655277777777778\n",
            "  tuple_cardinality_predictions_TAPAS: 0.19800000000000006\n",
            "  tuple_constraint_predictions_TAPAS: 0.1111111111111111\n",
            "  tuple_order_predictions_TAPAS: 0.37777777777777777\n",
            "----------------------------\n",
            "SQL Tag: SELECT\n",
            "  cell_precision_predictions_TAPAS: 0.2\n",
            "  cell_recall_predictions_TAPAS: 0.2\n",
            "  tuple_cardinality_predictions_TAPAS: 0.128\n",
            "  tuple_constraint_predictions_TAPAS: 0.08\n",
            "  tuple_order_predictions_TAPAS: 0.0\n",
            "----------------------------\n",
            "SQL Tag: SIMPLE_AGGR\n",
            "  cell_precision_predictions_TAPAS: 0.5172413793103449\n",
            "  cell_recall_predictions_TAPAS: 0.5172413793103449\n",
            "  tuple_cardinality_predictions_TAPAS: 0.6930862068965516\n",
            "  tuple_constraint_predictions_TAPAS: 0.4482758620689655\n",
            "  tuple_order_predictions_TAPAS: 0.0\n",
            "----------------------------\n",
            "SQL Tag: WHERE\n",
            "  cell_precision_predictions_TAPAS: 0.646225\n",
            "  cell_recall_predictions_TAPAS: 0.6526875000000001\n",
            "  tuple_cardinality_predictions_TAPAS: 0.5715125000000001\n",
            "  tuple_constraint_predictions_TAPAS: 0.325\n",
            "  tuple_order_predictions_TAPAS: 0.0\n",
            "----------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spider_test_stats=calculate_and_log_stats(None,None,spider_test_pd,valid=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWt5jQvNJL4R",
        "outputId": "50c9fd2a-2b34-4057-cbd5-adfd6c092a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1045. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1043. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1044. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input for row 553 was too long and was skipped.\n",
            "The input for row 559 was too long and was skipped.\n",
            "The input for row 560 was too long and was skipped.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1034. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1035. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1036. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1039. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input for row 577 was too long and was skipped.\n",
            "The input for row 578 was too long and was skipped.\n",
            "The input for row 579 was too long and was skipped.\n",
            "The input for row 580 was too long and was skipped.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1142. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1143. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1086. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1085. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input for row 962 was too long and was skipped.\n",
            "The input for row 963 was too long and was skipped.\n",
            "The input for row 964 was too long and was skipped.\n",
            "The input for row 965 was too long and was skipped.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1099. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1104. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1082. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1081. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1081. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1081. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1135. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1135. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1084. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input for row 984 was too long and was skipped.\n",
            "The input for row 985 was too long and was skipped.\n",
            "The input for row 986 was too long and was skipped.\n",
            "The input for row 987 was too long and was skipped.\n",
            "The input for row 988 was too long and was skipped.\n",
            "The input for row 989 was too long and was skipped.\n",
            "The input for row 994 was too long and was skipped.\n",
            "The input for row 995 was too long and was skipped.\n",
            "The input for row 996 was too long and was skipped.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1084. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input for row 997 was too long and was skipped.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating QA tests:  46%|████▌     | 97/211 [00:00<00:00, 189.80it/s]WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "Evaluating QA tests:  83%|████████▎ | 176/211 [00:00<00:00, 175.91it/s]WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "Evaluating QA tests:  93%|█████████▎| 197/211 [00:01<00:00, 183.25it/s]WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "Evaluating QA tests: 100%|██████████| 211/211 [00:01<00:00, 181.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "General Model Performance Metrics for epoch:None\n",
            "avg_loss: None\n",
            "general_avg_cell_precision: 0.5502417061611373\n",
            "general_avg_cell_recall: 0.5545355450236967\n",
            "general_avg_tuple_cardinality: 0.48390995260663494\n",
            "general_avg_tuple_constraint: 0.2906777251184834\n",
            "----------------------------\n",
            "Grouped Stats by SQL Tag:\n",
            "SQL Tag: HAVING\n",
            "  cell_precision_predictions_TAPAS: 0.3114166666666667\n",
            "  cell_recall_predictions_TAPAS: 0.48608333333333337\n",
            "  tuple_cardinality_predictions_TAPAS: 0.48808333333333337\n",
            "  tuple_constraint_predictions_TAPAS: 0.27775\n",
            "  tuple_order_predictions_TAPAS: 0.0\n",
            "----------------------------\n",
            "SQL Tag: ORDERBY\n",
            "  cell_precision_predictions_TAPAS: 0.7129444444444444\n",
            "  cell_recall_predictions_TAPAS: 0.6655277777777778\n",
            "  tuple_cardinality_predictions_TAPAS: 0.19800000000000006\n",
            "  tuple_constraint_predictions_TAPAS: 0.1111111111111111\n",
            "  tuple_order_predictions_TAPAS: 0.37777777777777777\n",
            "----------------------------\n",
            "SQL Tag: SELECT\n",
            "  cell_precision_predictions_TAPAS: 0.2\n",
            "  cell_recall_predictions_TAPAS: 0.2\n",
            "  tuple_cardinality_predictions_TAPAS: 0.128\n",
            "  tuple_constraint_predictions_TAPAS: 0.08\n",
            "  tuple_order_predictions_TAPAS: 0.0\n",
            "----------------------------\n",
            "SQL Tag: SIMPLE_AGGR\n",
            "  cell_precision_predictions_TAPAS: 0.5172413793103449\n",
            "  cell_recall_predictions_TAPAS: 0.5172413793103449\n",
            "  tuple_cardinality_predictions_TAPAS: 0.6930862068965516\n",
            "  tuple_constraint_predictions_TAPAS: 0.4482758620689655\n",
            "  tuple_order_predictions_TAPAS: 0.0\n",
            "----------------------------\n",
            "SQL Tag: WHERE\n",
            "  cell_precision_predictions_TAPAS: 0.646225\n",
            "  cell_recall_predictions_TAPAS: 0.6526875000000001\n",
            "  tuple_cardinality_predictions_TAPAS: 0.5715125000000001\n",
            "  tuple_constraint_predictions_TAPAS: 0.325\n",
            "  tuple_order_predictions_TAPAS: 0.0\n",
            "----------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spider_test_stats=calculate_and_log_stats(None,None,spider_test_pd,valid=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "St9kfnYmnAPV",
        "outputId": "f9a43ede-06bb-48e7-a3d1-ba6aef453710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1045 > 1024). Running this sequence through the model will result in indexing errors\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1045. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1043. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1044. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input for row 553 was too long and was skipped.\n",
            "The input for row 559 was too long and was skipped.\n",
            "The input for row 560 was too long and was skipped.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1034. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1035. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1036. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1039. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input for row 577 was too long and was skipped.\n",
            "The input for row 578 was too long and was skipped.\n",
            "The input for row 579 was too long and was skipped.\n",
            "The input for row 580 was too long and was skipped.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1142. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1143. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1086. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1085. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input for row 962 was too long and was skipped.\n",
            "The input for row 963 was too long and was skipped.\n",
            "The input for row 964 was too long and was skipped.\n",
            "The input for row 965 was too long and was skipped.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1099. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1104. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1082. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1081. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1081. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1081. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input for row 984 was too long and was skipped.\n",
            "The input for row 985 was too long and was skipped.\n",
            "The input for row 986 was too long and was skipped.\n",
            "The input for row 987 was too long and was skipped.\n",
            "The input for row 988 was too long and was skipped.\n",
            "The input for row 989 was too long and was skipped.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1135. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1135. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1084. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n",
            "<ipython-input-18-d380f16ec0a8>:44: UserWarning: After tokenization the input is longer than 1024 tokens: 1084. the input will be skipped\n",
            "  warnings.warn(f'After tokenization'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input for row 994 was too long and was skipped.\n",
            "The input for row 995 was too long and was skipped.\n",
            "The input for row 996 was too long and was skipped.\n",
            "The input for row 997 was too long and was skipped.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating QA tests:  41%|████      | 86/211 [00:05<00:08, 15.47it/s]WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "Evaluating QA tests:  51%|█████     | 108/211 [00:05<00:03, 32.67it/s]WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "Evaluating QA tests:  80%|████████  | 169/211 [00:07<00:01, 25.72it/s]WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "Evaluating QA tests:  90%|████████▉ | 189/211 [00:08<00:00, 28.39it/s]WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "WARNING:root:Table should be a list of list:  [[\"wales\", \"scotland\"], [\"england\"]]. Returning zero\n",
            "Evaluating QA tests: 100%|██████████| 211/211 [00:09<00:00, 23.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "General Model Performance Metrics for epoch:None\n",
            "avg_loss: None\n",
            "general_avg_cell_precision: 0.5216066350710901\n",
            "general_avg_cell_recall: 0.5181990521327013\n",
            "general_avg_tuple_cardinality: 0.46490521327014206\n",
            "general_avg_tuple_constraint: 0.265085308056872\n",
            "----------------------------\n",
            "Grouped Stats by SQL Tag:\n",
            "SQL Tag: HAVING\n",
            "  cell_precision_predictions_TAPAS: 0.20833333333333334\n",
            "  cell_recall_predictions_TAPAS: 0.3333333333333333\n",
            "  tuple_cardinality_predictions_TAPAS: 0.48299999999999993\n",
            "  tuple_constraint_predictions_TAPAS: 0.16666666666666666\n",
            "  tuple_order_predictions_TAPAS: 0.0\n",
            "----------------------------\n",
            "SQL Tag: ORDERBY\n",
            "  cell_precision_predictions_TAPAS: 0.6855\n",
            "  cell_recall_predictions_TAPAS: 0.61875\n",
            "  tuple_cardinality_predictions_TAPAS: 0.15911111111111115\n",
            "  tuple_constraint_predictions_TAPAS: 0.07222222222222223\n",
            "  tuple_order_predictions_TAPAS: 0.3777777777777778\n",
            "----------------------------\n",
            "SQL Tag: SELECT\n",
            "  cell_precision_predictions_TAPAS: 0.14364000000000002\n",
            "  cell_recall_predictions_TAPAS: 0.11736\n",
            "  tuple_cardinality_predictions_TAPAS: 0.02136\n",
            "  tuple_constraint_predictions_TAPAS: 0.0\n",
            "  tuple_order_predictions_TAPAS: 0.0\n",
            "----------------------------\n",
            "SQL Tag: SIMPLE_AGGR\n",
            "  cell_precision_predictions_TAPAS: 0.4827586206896552\n",
            "  cell_recall_predictions_TAPAS: 0.4827586206896552\n",
            "  tuple_cardinality_predictions_TAPAS: 0.6706724137931034\n",
            "  tuple_constraint_predictions_TAPAS: 0.4482758620689655\n",
            "  tuple_order_predictions_TAPAS: 0.0\n",
            "----------------------------\n",
            "SQL Tag: WHERE\n",
            "  cell_precision_predictions_TAPAS: 0.6411250000000001\n",
            "  cell_recall_predictions_TAPAS: 0.6516375\n",
            "  tuple_cardinality_predictions_TAPAS: 0.5892250000000001\n",
            "  tuple_constraint_predictions_TAPAS: 0.31666249999999996\n",
            "  tuple_order_predictions_TAPAS: 0.0\n",
            "----------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spider_stats_df = pd.DataFrame([spider_test_stats])\n",
        "spider_stats_df.to_csv('/content/drive/MyDrive/TAPEX_Ressources/Results/train_df_spider_100prct_Metrics_June_Corrected.csv', index=False)"
      ],
      "metadata": {
        "id": "Q7WDsjLEnDwP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}